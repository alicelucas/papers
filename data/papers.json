[{
  "_id": {
    "$oid": "603e6c62a603d992335c9317"
  },
  "authors": "Carion, N., Massa, F., Synnaeve, G., Usunier, N., Kirillov, A., & Zagoruyko, S.",
  "date": "2020",
  "journal": "European Conference on Computer Vision",
  "title": "End-to-end object detection with transformers",
  "__v": 0,
  "sections": {
    "why": "In most traditional CNN-based methods for object detection, the workflow isn’t end-to-end and involves some amount of “preprocessing” or “postprocessing.” \n\nFor example there are a lot of works (including R-CNN-based methods) that are going to pose \\nthe problem as first predicting a set of candidate boxes, and then classifying these boxes. These types of preprocessing are not ideal, because it makes the performance of the overall model very dependent on the engineering decisions made. \n\nPostprocessing” often includes the use of NMS methods to remove duplicate bounding box predictions. \\n\\nIn this work, the object detection workflow is much simpler. That’s why their paper is called “end-to-end” object detection. That’s a big thing. ",
    "what": "They choose to formulate the object detection problem as a set prediction problem. \n\nNote that they are not the first to approach the object detection problem as a set prediction problem. There exist methods where RNNs do that, but these methods are auto-regressive. This work here uses parallel decoding. (They make use of the recent work of parallel decoding with transformers)\n\nFurthermore, and that’s where there is the most novelty: they use an encoder-decoder transformer architecture with parallel decoding to model the inference. ",
    "how": "It’s a set prediction problem, so the target is going to be a set of size N, containing tuples of the (class, bounding box) of the instances in that image, and everything else in that set will be the “no object” label. This “no object” label can be seen as a way to pad the set. The architecture will be such that it outputs a set of that same size N. Again, the architecture is able to predict the “no object” label. The important thing to understand is that the target prediction is a set of N instances (most of them will correspond to “no object”), and the proposed prediction is a set of N instances as well.\n\nThe biggest challenge when formulating things as a set prediction problem is to match the prediction of the network to an instance in the target set. The way this is achieved here is with a “bipartite matching loss”. It uses the outputted classes and bounding box locations to try to match each prediction instance to another instance in the target set. Note that this results in a one-to-one matching, so we can be sure that multiple predicted bounding boxes will not be mapped to the same instance in the target set. That’s why NMS is not needed in this workflow. \n\nOnce the predictions are matched to the target instances, we use a Hungarian loss to actually train the neural network. This loss is very intuitive and similar to simple classification loss used in object detection. There is an extra novelty that they use a generalized IOU loss on the bounding box loss component, which helps making the loss a bit more scale invariant. \n\nThe user of a transformer architecture is the key distinguishing feature of their paper. Figure 2 in their paper shows the following:\n- An initial CNN, named “backbone” CNN is going to extract features from the initial image. There is going to be more channels and a smaller spatial extent. Then this feature map of size C x h x w is going to be flattened into a further compressed vector of size d x (hw). That’s the sequence that’s going to be fed at the input of the encoder. In some sense, each “time step” corresponds to a pixel in the image (with a feature vector representation of size d).\n- The encoder part of the transformer takes in this image sequence, in addition to positional encoding information. This is necessary because all information about position has been lost when we flattened the image and feed that to the encoder. The encoder then is going to output feature representations of those images. At this point, the encoder should have learned to focus on specific parts of the image to extract different instances.\n- The decoder part of the transformer is going to look at the extracted features from the encoder and output a set of bounding boxes. The decoder architecture also takes in “object queries”. It is necessary to input object queries to the decoder: to output N things, we need to input N things to it. These are learned during training. When combining information from feature maps from the object query, and feature maps from the encoder, then we can obtain bounding boxes.",
    "results": "The most interesting part of this paper is their section on visualizing the attention of the encoder and decoder networks.\n\nThey show that the encoder pays attention to the different instances in the image. Note that it does really well as separating close / overlapping instances. \n\nThe decoder clearly pays attention to the extremities of the object. It seems that it learns from the feature representation given by the encoder, and understands that extremities is what matters, and predicts a bounding box from that. \n\nAs a result of training, each “slot” (box) in the decoder becomes responsible for looking at a specific area of the image. This is shown in Figure 7. It seems like each slot is responsible for predicting a bounding box at a specific region of the image. \n\nIn terms of performance w.r.t RCNN and its different flavors, they show that they perform similarly (better for predicting large boxes, a bit worse for predicting small boxes). \n\nFinally, they show that they can easily apply their DETR model to the task of panoptic segmentation. All that is needed is a “mask head” (a CNN) to be appended to the features outputted by the decoder. The resulting network can be trained for panoptic segmentation (jointly, or not). In most cases, they do better than the previous SoTA PanopticFPN (which is based on a mask RCNN with an instance and segmentation branch) re-trained on their dataset. "
  },
  "labels": [
    "Object detection",
    "Transformers",
    ""
  ]
},{
  "_id": {
    "$oid": "603e6c9ba603d992335c9318"
  },
  "authors": "Chen, T., Kornblith, S., Norouzi, M., & Hinton, G.",
  "date": "2020",
  "journal": "International conference on machine learning ",
  "title": "A simple framework for contrastive learning of visual representations.",
  "__v": 0,
  "sections": {
    "why": "The contrastive learning idea (of making representations of an image agree with each other) is not new; it dates back to a Becker & Hintor paper in 1992. But previously proposed contrastive learning requires specialized architecture or a memory bank. \nThis new framework removes the need for that. They spell out the necessary elements for having a successful simple contrastive learning procedure.  ",
    "what": "SimCLR algorithm is composed of three steps: (1) transformation, (2) representation, (3) projection. The latter two steps are done by a neural network. The projection neural network is typically one or two linear (or non-linear) layer.\n",
    "how": "In this contrastive learning setup, an image is transformed in two different ways. These two resulting data points are called a “positive pair”. Each will be inputted to the same neural network (composed of a representation network and a projection head network) that will extract a feature vector for each of them. \n\nWe want the extracted representations of two transformed images to agree with each other.   The contrastive learning task will have a loss function that is going to maximize the “agreement” between these two feature vectors. \n\nWith these representations, we can then use these for linear classification. Or we can use the obtained network as an initial point for fine-tuning or transfer learning procedures.\n\nThe authors try out many different conditions, including different combinations of data augmentations, different architectures, and loss functions. \n\nThe end goal of experimenting with these different conditions is to determine which one leads to the best representations for a subsequent supervised learning task. They use a “linear evaluation protocol” which is going to train a classifier directly on these extracted representations, from the different settings. The better the final performance on the classification task, the better the representation is thought to be.",
    "results": "Learning representations in the unsupervised setting benefit from:\n- Composing augmentations, particularly the combination of random cropping and random color distortion. Unsupervised learning benefits more from this than supervised counterparts.\n- Augmentations can replace the design of complex architectures as previously done in contrastive learning.  For example, random cropping of  an image can easily provide adjacent viewers and global/local views for the object in that image. \n- Increasing depth and width. Unsupervised learning benefits more from this than supervised counterparts.\n- Adding a projection head improves the representation learned. If it’s a non-linear projection it’s better. However note that the representation used for the downstream task should be that of the layer before the projection head, not the one after (See Section 4.2). \n- The contrastive training loss that resulted in the best performance is the NT-Xent loss (Normalized, Temperature-scaled cross entropy loss). They find that the use of l2 normalization and temperature scaling is crucial for the resulting representation.\n- Training longer and larger batch sizes leads to better representations as well. That is because the network ends up seeing more negative examples, which facilitates convergence.\n- Compared with state-of-the-art (other methods for learning representations): they consistently outperform them for the linear evaluation mode and in the semi-supervised mode (in which the model is fine-tuned over few data points in ImageNet). "
  },
  "labels": [
    "Contrastive learning",
    "Unsupervised learning",
    ""
  ]
},{
  "_id": {
    "$oid": "603e6ce1a603d992335c9319"
  },
  "authors": "Bochkovskiy, A., Wang, C. Y., & Liao, H. Y. M.",
  "date": "2020",
  "journal": "arXiv preprint arXiv:2004.10934",
  "title": "Yolov4: Optimal speed and accuracy of object detection",
  "__v": 0,
  "sections": {
    "why": "The authors’s objective is to have an Object Detection model that works in the real world setting (for example for recommendation systems). Most works do not pay enough attention to the inference cost, which could in fact prevent these models to be implemented for real-world applications. \n\nMost researchers do not take that into account when developing new models, where accuracy is improved at the expense of inference cost.\n\nThe objective is to implement a model that works in real time, and only needs a conventional (normal) GPU for training and testing, and that works as well as the other, more costly methods.",
    "what": "Their search for the perfect model for real-time prediction is done by looking in-depth into all of the existing architectural and training design features that have been introduced in the literature, and shown to have a positive effect on the accuracy of the model. \n\nThey define the architecture as composed of individual components: a backbone, a neck, and a head. How each of these components can have a different number and different types of layers. \n\nThe training design mechanisms are classified into two different types of groups, the “Bag of freebies,” and the “Bag of specials.”  “Bag of freebies” lead to better accuracy with no increase in inference cost. These include data augmentation, special example mining, or better loss functions. “Bag of specials” leads to better accuracy, but may come with an inference cost increase. These include special attention modules, feature integration mechanisms, or the design of activation functions. \n",
    "how": "They are going to (1) study the different existing architectural components, and (2) experiment with the different tools obtained from bag of freebies and bag of specials. \n\nIn addition to listing all of the BoF/BoS and experimenting with them, they (1) introduce new data augmentation methods and and (2) they change existing design methods such that they lead to more efficient training and detection (See Section 3.3). The resulting architecture and BoF/BoS selections are shown in the left column of Page 7. \n\nAll of their different combinations are tested by using ImageNet’s (ILSVRC) dataset and the COCO test dataset. For all of their experiments, they only use one GPU for training.\n",
    "results": "They choose CSPDarkNet53 as their backbone. For the neck, they find that SPP (Spatial Pyramid Pooling) blocks and a PANet aggregation neck works best. Finally, their head, responsible for the detection, is chosen to be YOLOv3. Their resulting model is named YOLOv4.  \n\nTheir resulting YOLOv4 becomes the fastest and most accurate detector proposed in the literature.\n\nNote that they provide a link to their GitHub repo which includes a lot of documentation. \n"
  },
  "labels": [
    "Object detection",
    "Image classification"
  ]
},{
  "_id": {
    "$oid": "603e6d51a603d992335c931a"
  },
  "authors": "Wu, S., Rupprecht, C., & Vedaldi, A.",
  "date": "2020",
  "journal": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition",
  "title": "Unsupervised learning of probably symmetric deformable 3d objects from images in the wild",
  "__v": 0,
  "sections": {
    "why": "There is a vast literature on the topic of 3D reconstruction, which includes non-learning-based and learning-based methods. In the learning-based approaches, it’s often the case that the supervision signal uses 3D ground truth, or videos, or 2D keypoint annotations, etc. \n\nThis work never uses 3D labels (or related labels) as part of its supervision. It is completely unsupervised. It uses a single-view input image as its input (whereas other works utilize multiple views of that image to reconstruct the 3D). \n",
    "what": "The learning problem is reconstructive. Given an input image, the network’s task is going to reconstruct the input image. Note that this network is faced with an inverse problem: given the observed input image, the objective is to recover the underlying parameters that make its 3D structure. \n\nAn additional guidance principle in their work is that of symmetry. They assume that many object categories have, at least in principle, a symmetric structure. This assumption will be incorporated in the overall loss function. \n",
    "how": "The input is the 2D image, and the output are the four parameters (depth, albedo, viewpoint and lighting). This corresponds to a 3D internal representation, from which a 3D image can be rendered.\n\nThe symmetry is learned via two mechanisms:\n- Once we have the outputs (albedo, light, depth, viewpoint) of the individual neural networks, It is possible to render a symmetric reconstruction by flipping, across the vertical axis, the albedo and depth maps. Symmetry is then learned by enforcing that this reconstructed symmetric image, once projected back to 2D, is equal to the input image. \n- Also, they augment the model to reason about potential lack of symmetry in objects, by having it predict a probability map of whether a given pixel has a symmetric counterpart in the image. This probability map is used when computing the loss term.\n\nThe learning objective is reconstructive: the model is trained to reconstruct the input image. It learns to decompose the input image into a (depth, albedo, viewpoint and lighting) parameters, and input these to a renderer that outputs a 3D image. The loss component is going to apply a projection to the resulting 3D image and enforce that it matches the input image. \nThere are two loss terms in the overall loss: the reconstructive loss that enforces that the image from the predicted parameters is equal to the input image, and a loss that enforces that the image from the symmetrically predicted parameters is equal to the input image. \n",
    "results": "\nThey test their methods on human face datasets, cat face dataset, and synthetic cars datasets.\nAs a result of training, the resulting autoencoder internally decomposes the image into albedo, depth, illumination and viewpoint, without direct supervision for any of these factors. \n\nTheir evaluation metrics use the ground truth depth maps that they have in their dataset. Given the depth maps predicted by their model, they can compute the “scale-invariant depth error” (SIDE) metric as an evaluation metric. In addition, they use the “Mean Angle Deviation”, again computed from the depth maps.\n\nThey can also do a qualitative comparison with SOTA by just comparing the quality of the 3D reconstructions, see for example Figure 7.\n\nVia ablation studies, they show that symmetry and illumination are crucial cues to have the model converge to a good reconstruction. \n\nTheir model even outperforms a 3D reconstruction method that uses 2D keypoint supervision as additional input (whereas they only use the single viewpoint input image). "
  },
  "labels": [
    "3D reconstruction",
    "Unsupervised learning",
    "Auto-encoders"
  ]
},{
  "_id": {
    "$oid": "604ce20c36c8bc367058f276"
  },
  "title": "Momentum Contrast for Unsupervised Visual Representation Learning",
  "authors": "He, K., Fan, H., Wu, Y., Xie, S., & Girshick, R.",
  "date": "2020",
  "journal": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition",
  "sections": {
    "why": "Using contrastive learning has been shown to be a successful way to learn representations in an unsupervised manner, which can then lead to better performance when using these representations for supervised tasks. Their work is meant to provide a better training mechanism that leads to better representations. These representations can be used as a linear classification protocol, or for transferring tasks.",
    "what": "The contribution is in the training mechanism in the unsupervised learning task. In the traditional contrastive learning, the number of negative examples provided during training are limited by the small mini-batch size. Thus the learning is limited by small batch sizes. In this work, they keep the previously encoded keys in a queue of size K keys, much larger than a mini-batch of size N. By using such a large queue, we get a large negative set of samples, which results in better learning.\n",
    "how": "Each forward pass provides a new set of keys for the current mini-batch. In addition, we still keep the previous encoded K keys in the queue as additional negative examples when computing loss and gradients. The gradient of the query network f_q utilizes all of the negative samples that are in the queue. To update the keys network f_k, though, we do not use backpropagation as things would become intractable — think of all the keys that are in the queue, each of them coming from that f_k, it is too many gradients. But we still want to update the keys f_k network, otherwise it becomes quickly out-of-date (as in the case of memory bank contrastive learning) and we might as well be comparing apples and oranges during training. To do so, they use a simple momentum update, which uses the weights of the queries network that are regularly updated by backprop. (See Equation 2). \n\nAt the end of a forward pass, the current representations of that mini-batch are enqueued in the dictionary, the oldest is dequeued.\n\nTheir method can be seen as an enhanced procedure for contrastive methods that use memory bank. Like the memory-bank-based methods, they keep a representation of all samples in the data (to some extent). By using the momentum update rule, and regularly enqueuing and dequeuing keys, they update the encoder network that results in the representations of those samples stored in the queue, on the long term. \n\n",
    "results": "Under the linear classification protocol, they even do better than contrastive methods that use memory banks, which keep a representation of ALL samples in the dataset. \n\nThey show that they have a good tradeoff between number of parameters and accuracy. Even their encoders with a relatively small number of parameters lead to good results (see Table 1).\n\nThey use their methods to do feature transferring for image classification, and object detection tasks. The trained encoder is the backbone for that object detection task. It is fine-tuned on the ground-truth labels that we have. Note that in Section 4.2, they write about how the distribution of features resulting from contrastive loss learning are not the same as features typically obtained from supervised learning — thus the typical hyperparameters used to train the same backbones on supervised tasks would be different. They decide to keep the same hyperparameters, but instead normalize the unsupervised features learned. \n"
  },
  "__v": 0,
  "labels": [
    "Unsupervised learning",
    "Contrastive learning",
    ""
  ]
},{
  "_id": {
    "$oid": "604ce4ea36c8bc367058f27a"
  },
  "title": "Whole-cell segmentation of tissue images with human-level performance using large-scale data annotation and deep learning",
  "authors": "Greenwald, N. F., Miller, G., Moen, E., Kong, A., Kagel, A., Fullaway, C. C., ... & Van Valen, D.",
  "date": "2021",
  "journal": "bioRxiv",
  "sections": {
    "why": "Accurate whole cell segmentation is crucial for understanding and quantifying spatial organization of tissues.  Inaccuracies at this stage can have bad consequences on the downstream analysis pipeline.  \n\nCell segmentation in tissue is a difficult talk for multiple reasons. These reasons include that cell morphology can vary widely from one tissue to another, and cell density is also often different (from densely packed to rare). \n\nDeep learning applied to bioimage analysis has become the popular approach. But for the task of whole-cell segmentation in tissue, very few annotated datasets exist for whole cell segmentation: most public datasets are for nuclei segmentation only (nucleis are easier to annotate). As a result, there does not exists any good DL model for whole cell segmentation. ",
    "what": "Their work introduces a new dataset, TissueNet, that contains whole-cell and nuclear annotations for tissue image. It contains more than 1 million of whole cell and nuclear annotations from nine different organs.\n\nUsing this dataset, they train a model named Mesmer. They integrate it as part of the DeepCell web app, making it easily accessible to biologists who wish to try it. \n",
    "how": "The TissueNet dataset was gathered using a supervised crowdsourcing, human-in-the-loop pipeline. This approach consists of multiple phases. In Phase 1, an expert annotates a small number of cells (~80), which are then used to train a preliminary model. In phase 2, given the output of the preliminary model, crowdsourced annotations (non-experts) make corrections. The corrected annotations are inspected by experts, and then added to the training dataset which is then used again to train the preliminary omdel. This continues with the model being re-trained with this new dataset, corrections are made again, verified by experts, etc. At the third phase, the model can generate predictions without any human supervision. This approach results in a huge dataset compared to what is already published: more than 1 million annotations.\n\nDeepCell Label is their platform for performing these annotations in the loop.\n\nThe Mesmer model is based on the PanopticNet architecture. Instead of the traditional two semantic heads for PanopticNet, their Mesmer has four semantic heads: two for nuclear segmentation, two for cell segmentation. They share the common ResNet backbone and FPN. \n\nThe input to Mesmer has two channels: a nuclear image (e.g., DAPI), and a membrane/cytoplasm image (e.g. CD45). The output of Mesmer is the boundary prediction and centroid predictions for every nucleus and cell. This is fed to the watershed algorithm for the final instance segmentation mask. \n",
    "results": "Compared with other deep learning models for tissue segmentation (e.g. FeatureNet, CellPose), they find that their model is faster and more accurate. Especially when compared with CellPose, which involves a lot of post-processing.\n\nThe model predictions are good across a range of tissue types. Errors were unbiased: errors were both on cells that were large, and small. They found that their generalist model (Mesmer) was competitive with specialist models, those that were trained on a specific tissue data type. \n\nHuman-level performance on cell segmentation. \n\nThey apply their Mesmer models on two research applications: (1) Quantifying cell morphology change during human pregnancy, (2) Segmentation for accurate downstream analysis of tissue imaging data. \n"
  },
  "__v": 0,
  "labels": [
    "Cell segmentation",
    "Bioimage analysis",
    "Instance segmentation",
    "Crowdsourcing",
    ""
  ]
},{
  "_id": {
    "$oid": "605609babb0c5c8013d1b19c"
  },
  "title": "Cellpose: a generalist algorithm for cellular segmentation",
  "authors": "Stringer, C., Wang, T., Michaelos, M., & Pachitariu, M.",
  "date": "2021",
  "journal": "Nature Methods",
  "sections": {
    "why": "Accurate cell segmentation is a crucial part of research, including segmentation of the cell border — not just nucleus. \n\nThe DSB 2018 Challenge gathered a large dataset of diverse nuclei with the objective to allow people to train a model on this dataset, a model that generalizes well across the different types of nuclei. A model based on Mask RCNN ended up doing very well. \n\nThe work of these authors has the same objective: train a generalist model that can accurately segment a diverse set of cell images. ",
    "what": "Models in the past have attempted cellular segmentation, too, but these methods don’t generalize well. \n\n The authors hypothesize that the failures of prior work is due to not using the right representation for cells and networks can not capture complicated shapes. This work changes the training task from predicting a segmentation map to instead predict a vector field map. The segmentation maps are obtained from post-processing the output of the neural network. \n\nThey make their CellPose model available online, in a web app, but also as a downloadable app with a GUI. ",
    "how": "The main contribution is in changing the predictive task of the DNN and using vector field representations of the images as the ground-truth labels. The neural network does not learn the segmentation task, but instead learns to predict a flow field by outputting (1) the horizontal gradient of the image, (2) the vertical gradient of the image, (3) the probability which indicates if a given pixel is part of a cell. Supposedly this should allow the network to extract a more robust representation of cells. \n\nGiven this predicted flow field, we can lead each pixel of a cell towards the center of that cell. For each pixel, they run a dynamical system starting at that pixel location and follow the spatial derivative specified by the horizontal and vertical maps. \n\nTo obtain the training data (the flow maps), a heat diffusion simulation is applied on each masks obtained by experts. \n\nThe neural network is based on the UNet architecture. They train the neural network on a diverse set of images. Note that it’s a relatively small dataset (616 images) — contrast that with TissueNet.\n\nThey obtain 3D segmentation by running CellPose on XY, XZ, and YZ slice to then form a 3D vector flow field. Then they apply the dynamic system as usual, and obtain a 3D segmentation mask. \n\n",
    "results": "To compare the CellPose framework, they train the StarDist and Mask-RCNN models on their specialized and generalized data. In both the generalized and specialized settings, the CellPose consistently outperforms these two models. \n\nNote that their framework involves very time-consuming post-processing. To predict the mask from the flow field, for example, they do 200 iterations for each pixel in the image. Furthermore, their “test time enhancements” (see page 10) adds more computational time to the framework. \n"
  },
  "__v": 0,
  "labels": [
    "Cell segmentation",
    "Bioimage anlysis"
  ]
},{
  "_id": {
    "$oid": "605636aabb0c5c8013d1b19e"
  },
  "title": "Unsupervised feature learning via non-parametric instance discrimination",
  "authors": "Wu, Z., Xiong, Y., Yu, S. X., & Lin, D.",
  "date": "2018",
  "journal": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition",
  "sections": {
    "why": "This work is another method for unsupervised learning of representations, which can then be used directly for a discriminative task, or used for fine-tuning supervised tasks. The latter application is particularly important as it is often that ground-truth data is hard to get in applications.\n",
    "what": "They argue that a useful embedding can be learned in an unsupervised manner by teaching a neural network to discriminate between instances. The network can then learn similarities among instances. Instead of learning “what makes a class a class”, they are teaching the network to tell us “what makes an image an image?” \n\nThese representations can then be used to construct a memory bank with known labels, and do subsequent classification with K nearest neighbors. Or these representations can be fine-tuned for another task, or used in a linear prediction problem. They test their representations on semi-supervised learning tasks on ImageNet, and fine-tuning tasks for object detection on PASCAL VOC. \n",
    "how": "The task becomes the following: each instance is its own class. In some sense we now have a “supervised” classification problem. If there are 1.2 million images in ImageNet, then we train the neural network on 1.2 million classes. The network learns a representation that is a vector with 128 elements only. \n\nThe probability that a given feature vector corresponds to class i (instance i) is given by the softmax equation, which is based on the inner product between the feature vector and the vector representation of instance i (see Equation 2). Note that the classic softmax formulation involves an inner product between the weights vector and the vector representation of the instance, making the learned model dependent on the classes assumed at the time of training. In the non-parametric case in Equation 2, the probability any new feature vector belonging to a specific instance can be computed. \n\nThe obvious problem is that this is computationally intractable (to say the least). They use two tricks to alleviate these computational concerns: \n— The first trick is to approximate the full softmax with Noise Contrastive Estimation. NCE is not a new method — it has been used before in the literature to reduce computation, but they apply it to their problem. NCE reduces computational complexity from O(n) to O(1) per sample. A second they tricky \n- The second trick is to use proximal regularization methods to stabilize learning process. Their regularization enforces that the representation of a specific feature at time t - 1 is not too different from the new computed one at the current inference pass at time t. \n\nAt each step of the training, a forward pass is computed and a new representation for instance i is obtained. The representation of that instance in the memory bank is updated. The loss and gradients are computed (matching each instance in the batch to its corresponding instance class), and the network parameters are updated to produce a better representation at the next step. The batch size used during training is 256 (but the class vector is 1.2 millions).\n\nThe resulting memory bank is not as large as one might think. Since they set the features of images to 128, the memory bank in the end is only 600 MB.",
    "results": "They test their representation in two different settings: (1) Linear SVM on intermediate features of their network, from conv1 to conv5 and (2) KNN on the output features (size 128). In this second test case, they first compute its feature representation with the neural network, and then compare it against the embeddings of all the images in the memory bank, using the cosine similarity function. The top k nearest neighbors are used to make the decision via weighted voting. \n\nThey show that their learned features does not work well in the linear classification setting, when a SVM is trained to do a classification directly on these features. This is expected, as the learned features do not correspond to semantic features. Hence the SVM is not working on a linearly separable space where classes have been separated from one another. It makes sense that KNN would be much more appropriate for this application (and convenient as there are only 128 features). Under the KNN test setting, their method outperforms the other unsupervised methods. \n\nSemi-supervisd setting is when we first learn from a large unlabelled data (as they did) and they use a small amount of labeled data to fine-tune the model. They train only on a small amount of data of ImageNet, from 1% to 20%. They compare their methods against other pre-training methods called “Split-brain” and “Colorization”. They consitently outperform both of them. \n\nFinally, they apply their learned features in the Object Detection setting, initializing the backbone networks with their learned weights and fine-tuning these for the object detection task. They compete well with the other unsupervised methods who have also finetuned on Object Detection tasks. If choosing ResNet-50 as their backbone, they outperform the other unsupervised methods. "
  },
  "__v": 0,
  "labels": [
    "Unsupervised learning",
    "Supervised learning",
    "Memory bank"
  ]
},{
  "_id": {
    "$oid": "605f4a8f802cf683982d6798"
  },
  "title": "Big Self-Supervised Models Advance Medical Image Classification",
  "authors": "Azizi, S., Mustafa, B., Ryan, F., Beaver, Z., Freyberg, J., Deaton, J., ... & Norouzi, M.",
  "date": "2021",
  "journal": "arXiv preprint arXiv:2101.05224",
  "sections": {
    "why": "Classification for medical imaging tasks is no easy feat because getting labeled data in medical applications is a pain. We need to develop Deep-Learning based methods that do not require lots of labeled data to work well. They focus on the case in which we do pretraining to learn representations, and then fine-tune the models on a task using few labels.",
    "what": "\nThe authors investigate in detail what types of pre-training can help for learning good representation which can then in term provide high accuracy for classification tasks on medical image data, when few labels are available.  \n\nThey leverage the power of pre-training and study its impact on subsequent medical imaging classification tasks. They compare supervised pre-training (for example on the large labeled ImageNet dataset) and unsupervised pre-training (using contrastive learning).\n\nOne of the take-aways from this paper is that supervised pre-training on natural image datasets is not an ideal pre-training approach for subsequent medical tasks, and unsupervised pre-training should be favored instead, to lead to better representations that lead to better generalization and are more label-efficient in subsequent fine-tuning. ",
    "how": "Their proposed pipeline is the following. First, they do unsupervised learning on ImageNet or on the medical data using the classical contrastive learning approach, SimCLR. This approach uses data augmentation to generate positive pairs. Each image is augmented twice using either random crop, color distortion, or Gaussian blur. The representation obtained from a ResNet-like architecture is fed to a non-linear projection head, and the resulting representation goes to a contrastive loss based on cosine similarity.\n\nNext, if possible, they continue the unsupervised learning phase with multi-instance contrastive learning (MICLe) for additional self-supervised training on the medical data. In this case, the positive pairs are naturally obtained by having taken a given pathology from different viewing angles, lighting conditions, or different body parts.\n\nThird, with these representations, fine-tune your ResNet-like architecture on the few labels that are available for this medical classification task. ",
    "results": "They find that best results are obtained when both the ImageNet and the task-specific medical image dataset are used, rather than one or the other individually.\n\nFurthermore, they find that using MICLe instead of SimCLR consistency improves the resulting model performance.\n\nInterestingly, they show that doing supervised classification pre-training on the labeled ImageNet does not improve the performance as well as unsupervised pre-training does. It may be that the representations learned are too task-specific and do not generalize well. \n\nFinally, self-supervised models are most robust to domain shifts after having fine-tuned on a specific domain. This is in contrast with using a supervised pre-training approach on ImageNet, which does not do well under domain shifts. In additional, they show that using self-supervised pretraining instead of supervised pre-training results in needing less labels during the finetuning stage. "
  },
  "__v": 0,
  "labels": [
    "Unsupervised learning",
    "Semi-supervised learning",
    "Fine-tuning",
    "Contrastive learning",
    ""
  ]
},{
  "_id": {
    "$oid": "6064de55500bd84dbbc5d57d"
  },
  "title": "Cell detection with star-convex polygons",
  "authors": "Schmidt, U., Weigert, M., Broaddus, C., & Myers, G.",
  "date": "2018",
  "journal": "International Conference on Medical Image Computing and Computer-Assisted Intervention (pp. 265-273). Springer, Cham.",
  "sections": {
    "why": "Accurate cell and nuclei segmentation is an important and crucial step in research. Current object detection SOTA methods, such as Mask-RCNN, do not adapt well to shapes such as those of nuclei and cells.",
    "what": "In object detection, a typical top-down approach is to first predict regions of interest and then classify the objects in these found regions of interest. This is different from the bottom-up approach, where the image is first classified with a semantic map, and then different instances are extracted. \n\nThis paper employs the top-down approach. It argues that the typically used bounding boxes are not appropriate shapes to use when segmenting cells or nuclei. The post-processing step NMS will not work well if the underlying objects are not accurately represented by these bounding boxes. Instead, they argue that star-convex polygons is the more appropriate shape to use. ",
    "how": "Stardist’s architecture is a light-weight neural network based on U-Net. \n\nThe task of the network is to predict for each pixel: (1) the distance between that pixel and the nearest background pixel, which can be seen as a probability of belonging to an instance, and (2) the distance to the object boundary along each of the n predefined radial directions. \n\nA binary cross-entropy loss function is used to predict the object probabilities, and a MAE loss for predicting the radial distances. \n\nThey apply NMS to only retain polygons in a certain region with the highest object probabilities. They only consider polygons associated with pixels above a certain object probability threshold. \n",
    "results": "They compare their results with a UNet trained for a simple semantic classification task. They test on synthetic datasets of cells and nuclei, and on nuclei dataset from DSB 2018. On the real dataset of DSB 2018, StarDist outperforms all methods. StarDist does particularly well at not merging cells or not missing cells. This is supposedly due to the use of a star-convex polygon which creates less problems when using NMS.\n\nI wish they had provided a comparison of using their method with the same method, but with the only difference being the prediction of a bounding box instead of a star polygon shape. It’s difficult to know here what is the effect of predicting star-convex shapes. They do compare with Mask-RCNN, but they use the open-source implementation of it, so the comparison may not be valid.\n\nTwo of their test datasets are synthetic, which is a bit of a disappointment for seeing the performance on cell data. A real cell test dataset should be used. "
  },
  "__v": 0,
  "labels": [
    "Object detection",
    "Cell segmentation",
    "Bioimage analysis",
    ""
  ]
},{
  "_id": {
    "$oid": "6064e29e500bd84dbbc5d57f"
  },
  "title": "Deep watershed transform for instance segmentation",
  "authors": "Bai, M., & Urtasun, R.",
  "date": "2017",
  "journal": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (pp. 5221-5229)",
  "sections": {
    "why": "This work proposes an end-to-end method for instance segmentation that combines classical image segmentation with learning-based image classification. \n\nThe current (in 2017) methods for Deep Learning segmentation usually involve a complex pipelines, for example involving Conditional Random Fields, or template matchings, or large RNNs. \n\nWatershed-based segmentation has the problem that it tends to over-segment the image. \nThis method should supposedly combine the best of both worlds. \n",
    "what": "Traditionally, the watershed transform is applied on the pre-computed gradients of the image. However, it is often difficult to find sharp gradients that represent well the boundaries between different instances. Instead, they propose to use a CNN that directly predicts the energy of the watershed transform, given an image.\n\nThe predicted transform is constrained to have each basin correspond to a single instance — this is useful to prevent over-segmentation frequently observed in watershed (See Figure 2 for a great illustration). \n",
    "how": "They aid the network by defining an intermediate task: have the network predict a “distance transform”, which tells us for each pixel how far it is from its instance’s boundary. This is performed with a group of layers named “DirectionNet” This is then passed to the next layers of the network, “Watershed Transform Net”, that predict the energy of the watershed transform. \n\nDirectionNet is pre-trained to estimate the direction of descent of the energy at each pixel. This results in a 2-channel unit vector map, where each pixel has a unit vector pointing to a specific direction. \n\nThis direction map is then fed to Watershed Transform Net, which outputs a discretized watershed transform map with 16 possible energy values. The higher the energy value at a given pixel, the more the pixel is located at the interior of the instance. Low energy values are for pixels near the boundary of the instance. With a single level cut on the energy map, they can obtain the segmented instances. \n\nThis idea of using an energy map reminds of StarDist’s approach in predicting for each pixel, a distance to its instance boundary. \n\nThe input to the network is the image AND its semantic segmentation map. Note that this is an extra requirement that can be seen as a limitation. \n\nEach sub-network is pre-trained independently, and then the whole system is fine-tuned. ",
    "results": "They test their method on the Cityscapes Instance Segmentation benchmark. This is a difficult dataset because objects are often included, and there usually are dozens of instances in a single image. \n\nThey significantly outperform the other methods that they compare to. "
  },
  "__v": 0,
  "labels": [
    "Image segmentation",
    "Instance segmentation"
  ]
},{
  "_id": {
    "$oid": "606613eac8b4c721cc1f28f6"
  },
  "title": "Deep Feedback Inverse Problem Solver",
  "authors": "Ma, W. C., Wang, S., Gu, J., Manivasagam, S., Torralba, A., & Urtasun, R.",
  "date": "2020",
  "journal": "In European Conference on Computer Vision (pp. 229-246). Springer, Cham.",
  "sections": {
    "why": "There exists a family of problems called inverse problems, which are difficult to solve: given an observation, the objective is to recover the original input to the forward problem.\n\nThere exists two main approaches for solving inverse problems: analytical-based approaches, in which human knowledge is uses to regularize or constrain the solution and formulate an optimization problem; and deep-learning based approach, where the objective is to learn a mapping from the observation to the source. \n\nAnalytical-based approaches can be difficult to design, usually require many iterations to converge, and are sensitive to initialization. Deep Learning-based approaches often produce results that are not realistic or incompatible with the real observations. ",
    "what": "When solving the inverse problem analytically, there generally is a feedback mechanism during the algorithm that checks that we satisfy f(x*) = y, where x* is the predicted unknown at time t of the optimization, f() is the feedback mechanism, and and y is the observation. \n\nThe objective of this work is to use Deep Neural networks as part of the optimization algorithm. ",
    "how": "In a traditional optimization scheme, the update rule is given by x(t+1) = x(t) + g(xt, yt, y), where g() is an update function analytically derived from the energy function defined in the optimization objective function. In this work, g() is implemented by a neural network. Thus the neural network g() can be seen as computing a residual between the previous solution x(t) and the new solution x(t+1). By having yt and y as its input, the neural network can compute how well the currently predicted solution x matches the known forward model, and output a residual based on that. \n\nIn some sense, the neural network learns how to conduct an iterative update over the current solution, based on the known forward process and the current observation and the “ground-truth” observation.\n\nTo make this clear, the authors are not using the neural network as a direct mapping from y to x and are instead still using traditional analytical techniques. The main difference is that the update function g(), usually derived by hand, is implemented and learned by a deep neural network. \n",
    "results": "They apply their methods to three applications: pose estimation, illumination estimation and inverse kinematics. Comparing their approach with a method that does not use DL to implement the feedback update, they do better. For pose estimation, their method seems to be more robust to extreme poses. "
  },
  "__v": 0,
  "labels": [
    "Inverse problems",
    "Iterative optimization"
  ]
},{
  "_id": {
    "$oid": "606dd78dd25befccf8fbae9e"
  },
  "authors": "Kirillov, A., Girshick, R., He, K., & Dollár, P.",
  "date": "2019",
  "journal": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 6399-6408).",
  "title": "Panoptic feature pyramid networks",
  "sections": {
    "why": "The current SoTA for semantic segmentation is a FCN with dilated convolutions (also called atrous convolutions), and for instance segmentation it is a Mask-RCNN with a Feature Pyramid Network (FPN) backbone. \n\nThis work addresses the task of panoptic segmentation, which is the joint task of doing instance segmentation and semantic segmentation. ",
    "what": "In the recent COCO and Mapillary Recognition competition on panoptic segmentation, every competitive entry ended up using a separate network for each of the tasks, with no shared computation. The previous literature that has tried to unify instance and semantic segmentation models have resulted in loss of performance in either of those tasks.\n\nWe are thus in need of a single model that can do both tasks simultaneously, but still perform competitively with as SoTA on each of those tasks. \n\nThis work therefore unifies these methods at the architectural level, by designing a layer which will be shared across networks. Their resulting model is a simple architecture that matches accuracy with SoTA for both tasks. The network simultaneously generates region-based outputs and dense-pixel outputs.",
    "how": "The proposal is to start with the Mask R-CNN with FPN backbone (SoTA in instance segmentation). On top of these FPN features, they append an instance segmentation branch and a semantic segmentation branch. They make no change at all to the FPN backbone. The changes needed to get the semantic segmentation branch are minimal. \n\nEach of these branches share the same Feature Pyramid Network (FPN) backbone. Their resulting model is named Panoptic FPN. \n\nThe FPN backbone utilizes features at different resolutions. FPN generates a pyramid of features, scaling from 1/32 to 1/4 resolutions. It has the particularity that there are the same number of channel dimensions at each level. \n\nIn the instance segmentation branch, we attach a region-based object detector like faster RCNN on each of its pyramid level, to obtain candidate bounding boxes. This is made possible with the fact that that FPN backbone has the same number of channels at each level. Then by applying a FCN layer on the resulting detected regions for binary segmentation, a segmentation map for each instance is obtained (This is essentially the approach taken by mask-RCNN). \n\nThe semantic branch is going to take in the FPN features and upsample them and transform them in several ways such that in the end we are left with several feature maps corresponding to the 1/4 resolutions. A final 1x1 convolution, upsampling, and softmax layer is generated for each pixel, providing us with the segmentation map. \n\nThe output of the Panoptic FPN is going to have (1) a single class label and (2) an instance id for each pixel in the image.\n\nTraining these two branches jointly is tricky: in this paper, they provide the necessary information regarding the loss balances, constructing mini-batches, adjusting learning rate schedules, and performing data augmentation. ",
    "results": "They evaluate their models on COCO and Cityscapes. They perform very well on instance segmentation, since the Mask RCNN + instance segmentation branch is essentially what SoTA does already. But they also do perform very well on semantic segmentation, as it is competitive as SoTA performing dilation based FCNs (such as DeepLabV3). This is observed when the network is trained for solving the two tasks independently, and when trained for solving the two tasks at once (panoptic segmentation). The latter result is significant because that means that we get to do both tasks for half the compute. In addition, they outperform all other single-model entries in the recent COCO Panoptic Segmentation Challenge. Note that later (2020), the transformer for Object Detection is pubished and ouperforms PanopticFPN."
  },
  "labels": [
    "Feature Pyramid Networks",
    "Panoptic segmentation",
    "Dilated convolutions",
    "Multi-task learning"
  ]
},{
  "_id": {
    "$oid": "606f24e97f89b429787749f2"
  },
  "authors": "Snell, J., Swersky, K., & Zemel, R. S.",
  "date": "2017",
  "journal": "arXiv preprint arXiv:1703.05175",
  "title": "Prototypical networks for few-shot learning",
  "sections": {
    "why": " The few-shot classification problem is when a classifier must be adapted to be able to predict new classes not seen in the training set, given only a few examples of each new class.\n\nI emphasize here that typically in few-shot learning, no fine-tuning is preferable (no retraining of the classifier parameters whatsoever). The training objective is to learn a model that can output a robust representation even from just a few examples (the support set) of a new class, and use this representation for new query points. The training objective is not to provide a mapping from an input image to a class.",
    "what": " Many current approaches for few-shot learning employ a particular approach during training: mini-batches called “episodes” are used during training, where each episode is designed to mimic the few-shot task by subsampling classes as well as data points. This is called “episodic training”.  The popular “matching networks” (Vinyals et al.) uses episodic training. This work also uses episodic training.\n\nAnother SoTA method for few-shot classification is the meta-learner LSTM. If I remember correctly, a separate optimization problem is solved for each new unseen class.\n\nThis is interesting because it means that the task learned by the neural network during training is the few-shot classification task itself. \n\nTheir work introduces prototypical networks, a new method for one shot and few shot learning that is simpler and more efficient than recent meta-learning algorithms, and achieves SoTA performance.  The key contribution is that a simple “prototype” for each is class is assumed. The prototypical network is trained to produce the embedding space in which this prototype is compute.",
    "how": "They assume that each class has a single prototype representation. The class’s prototype is just the mean of its support set in an embedding space. The embedding space is learned by the prototypical neural networks. At test time, for a given query data point, we compute its representation in the embedding space using the trained prototypical network, and find the nearest class prototype.\n\nIn zero shot learning, a class comes with meta-data instead of labeled examples. For zero shot learning, the authors propose to learn a separate embedding of the meta-data itself. Given a new test point, we find the nearest class prototype, as in the few-shot learning case. \n\nThe training process mimics the few-shot problem, using episodes. During training, a subset of classes are sampled, and some images of that class are used as the support set to estimate a prototype (the mean of the embeddings of the support points), and other images of that class are used as the queries (classifying new, unseen example). With each gradient update, we update the embedding model f() so that it produces a better embedding space, hence a better prototype for whatever class we have at the next episode.\n\nThe key thing here is that during training, the support set is small and only a few classes are samples. Therefore the network is trained too solve the few-shot problem directly. ",
    "results": " They find that the choice of distance is critical:  the use of Euclidean distance greatly outperforms the generally used cosine similarity distance. \n\nThey seem to outperform matching networks and Meta-learner LSTM on the miniImageNet problem and the Omniglot dataset.\n\nThey obtain SoTA performance on the CUB-200 dataset for zero-shot learning."
  },
  "labels": [
    "Few-shot learning",
    "Zero-shot learning",
    "Episodic training"
  ]
},{
  "_id": {
    "$oid": "607996b71fe25dbe666b79f8"
  },
  "authors": "Chen, M., Radford, A., Child, R., Wu, J., Jun, H., Luan, D., & Sutskever, I.",
  "date": "2020",
  "journal": "International Conference on Machine Learning (pp. 1691-1703)",
  "title": "Generative pretraining from pixels.",
  "sections": {
    "why": " The current SoTA for image-related problems with Deep Learning has focused on solving the problems in a supervised fashion, utilizing large datasets that are available out there. \n\nRecently, however, there has been interest in learning representations of images in an unsupervised methods. These learned representations can then be used for linear classification, or for fine-tuning on limited amount of labeled dataset.",
    "what": " This paper is inspired by the success that NLP has had with unsupervised learning They borrow the GPT-2 architecture, more particularly its transformer decoder, and apply it on images instead of text modality. \n\nUnlike contrastive learning which use auxiliary tasks to learn representations, the representations are learned here using generative image modeling. They explicitly learn to model the distribution of pixels in an image.",
    "how": "They experiment with two different pre-training objective functions. The first one is the auto-regressive objective function, which  models p(x) as the product of all conditional probabilities p(x_i | x_{1…i-1}). In essence, each pixel is predicted by looking at all the other pixels previously predicted. The second objective function is BERT’s masked training objective, which is going to ask the model to predict masked pixels given unmasked pixels. \n\nThe input to their network is a flattened sequence corresponding to the pixels of the image. The decoder transformer produces a d-dimensional embedding for each input token (pixel), and on top of that has a stack of L blocks, each of them producing intermediate embeddings of size d. Because the image is flattened, the attention layer in the decoder is responsible for leaning to identify the 2D structure. Note the difference with DETR paper: they provide positional encodings to the input. \n\nBecause of the memory requirements of the transformer decoder, it would be impossible to feed in the whole sequence of length (224 x 224 x 3). To resolve this, the authors do two things: (1) they downsample the image to a smaller size, and (2) they switch from RGB to just a 9 bit color palette. The resulting context length has either (32 x 32), (48 x 48), or (64 x 64) elements. ",
    "results": " They design GPT models of different sizes. They train on the ImageNet ILSVRC 2012 dataset. \n\nWhen working on the linear probing task, they experiment with using features of different layers in the model (not necessarily the last one). They find that choosing middle layers have the best features for good linear probing performance, and performance decreases as we choose deeper features. Note that when fine-tuning on the classification task, they attach a classification head on the layer with the best representations ( a middle layer), not the last one. \n\nWhen fine-tuning on the CIFAR datasets, they do really well. That’s because those images have small resolution, and their model did well at learning the distribution of images of small spatial extent.\n\nThey don’t do great on fine-tuning on the ImageNet dataset.  Clearly a limitation of this setup is that because of the dense self-attention on the input, they are constrained to using low-resolution inputs. It’s difficult to learn high-quality representations in this case, and contrastive learning approaches (which are not constrained by this, since they use CNNs), will do better. "
  },
  "labels": [
    "Unsupervised learning",
    "Generative models",
    "Transformers"
  ]
},{
  "_id": {
    "$oid": "607c71e5d49955ea1c0483b9"
  },
  "authors": "Tuggener, L., Schmidhuber, J., & Stadelmann, T.",
  "date": "2021",
  "journal": "arXiv preprint arXiv:2103.09108",
  "title": "Is it Enough to Optimize CNN Architectures on ImageNet?",
  "sections": {
    "why": " This work establishes the prevalence of the ImageNet dataset in Deep Learning papers. When studies propose new SoTA architectures, the obtained results are based on the ImageNet dataset. But is the ImageNet dataset really appropriate to establish new baselines? Do the observed results generalize across other non-ImageNet visual datasets? This work provides an answer to this question. ",
    "what": " They gather multiple datasets and study whether improvements seen when testing on the ImageNet models generalize to other datasets. ",
    "how": "The datasets that they use for their study are: (1) images of concrete buildings, (2) images of reef habitats, (3) images of ImageNet, (4) dermatoscopic images, (5) images of powerlines in Turkey, (6) insects, (5) natural scene dataset, (7) Cifar10/100 dataset.  This is a very diverse set of datasets. \n\nThe architectural space that they investigate are based on a single architecture, AnyXNet, and its variants. The stem and the head are the same across all architecutres, but the body will vary. The body is made of standard residual bottlneck blocks. The body is parameterized by d the number of blocks, w the block width, b the bottleneck ratio and g the group width (this corresponds to how many parallel convolutions the total width is grouped into). The AnyXNet architecture’s design space has a total of 16 degrees of freedom, having four stages which each have four parameters (d, w, b, g).\n\nThey repeatedly sample the space to obtain a total of 500 architectures. All 500 models are trained on each dataset.  ",
    "results": " They show scatterplots for each dataset, where a point corresponds to the test error of a sampled architecture trained on that dataset vs the test error of a sampled architecture on ImageNet. A linear correlation in the scatterplot would imply that optimizing architectures based on ImageNet performance results in improved performance on other datasets as well. The scatterplots show that for some datasets, such as insect or Cifar100, there is a positive correlation. However, for other dataset (such as powerline or the natural scenes dataset), there is a negative correlation! In such cases, it’s not recommended to optimize things based on performance on ImageNet. \n\nWith additional studies, they show that a network depth and width found to be optimal on a certain dataset will not necessarily apply to another dataset — this is something important to keep in mind when doing transfer learning tasks.\n\nAnother interesting finding is that the generalizability of performance between datasets is related to the different number of classes in each dataset. Training on the ImageNet dataset with a subset of its classes (chosen to be similar to the target dataset) instead of its 1000 classes does lead to linear correlation. This implies that  one of the issues with the ImageNet dataset is its large number of classes compared to the number of classes included in the target dataset.  Thus if doing pre-training on ImageNet, it might be helpful to vary the number of classes and only use a subset of its classes.\n"
  },
  "labels": [
    "ImageNet",
    "Transfer Learning",
    ""
  ]
},{
  "_id": {
    "$oid": "607da4fe8d6d6324c209a82d"
  },
  "authors": " Donahue, J., Krähenbühl, P., & Darrell, T.",
  "date": " 2016",
  "journal": " arXiv preprint arXiv:1605.09782",
  "title": " Adversarial feature learning",
  "sections": {
    "why": " This work proposes a method for learning representation of images, in an unsupervised manner. These features can then be used for subsequent supervised classification tasks. ",
    "what": "The proposed method for learning representations is new: it makes use of Generative Adversarial Networks and their ability to sample natural images from a small latent space. \n\nThe general hypothesis is the following: if GANs can produce an image from a small latent space, can we also train a GAN to project an image to this small latent space? Can this learned encoding be used as a feature space for subsequent supervised tasks?",
    "how": " A traditional GAN consists of a generator network that takes in the vector z and produces an image G(z). In addition to this generator, the authors add another network, an encoder network, that will take in an image x and produce E(x), its features representation. \n\nIn this set-up, a discriminator will be trained for two tasks: (1) compare the produced image G(z) with the ground-truth images in the dataset, and (2) compare an encoded latent space vector E(x) and compare it with the original vector z. Similarly the generator and encoders’s task is to fool the discriminator. Training is done by computing gradient steps for D, G, and E for each mini-batch and updating all modules simultaneously. \n\nIn all experiments, the architecture E is based on the AlexNet network. \n\nAn alternative task for the encoder E that is explored in the paper is have a loss function that requires the resulting vector E(G(z)) to be close to the latent z. In the results section, they show that the features in an E() trained in that way also contain helpful representations but are not as efficient as what is obtained with training on E(x). \n",
    "results": " When fine-tuning for imageNet classification, they freeze the first N layers (they experiment with 1 to 5) of the BiGAN encoder and randomly initialize all the other weights to be retrained on ImageNet labels. They show that BiGAN is competitive with the other unsupervised learning methods that use the same AlexNet architecture.\n\nWhen fine-tuning for VOC detection, and segmentation, the AlexNet model is used as initialization for a Fast-RCNN and a FCN architecture. All of the layers are retrained. They show that the features learned are competitive with other existing methods for unsupervised representation.  "
  },
  "labels": [
    " Unsupervised learning",
    " GAN",
    ""
  ]
},{
  "_id": {
    "$oid": "6080920cb88504cf98c5c2ad"
  },
  "authors": " Van Gansbeke, W., Vandenhende, S., Georgoulis, S., Proesmans, M., & Van Gool, L.",
  "date": " 2020",
  "journal": "In European Conference on Computer Vision (pp. 268-285) ",
  "title": " SCAN: Learning to classify images without labels",
  "sections": {
    "why": " The objective of this work is to classify images without ever receiving labels. This is achieving by grouping images into semantically meaningful clusters",
    "what": " This is different from the unsupervised + finetuning works that you have been reading, where representations learned in an unsupervised way are then fine-tuned with existing labels. ",
    "how": " Their method is a two-step approach. First, they use a pretext task to learn feature representations of images in an unsupervised manner. Second, they use these feature representations as a prior for their clustering procedure. \n\nThe pretext task can be one of the pretext tasks already available in today’s literature, for example the task used for instance discrimination or for contrastive learning. The hypothesis here is that the similar features will be assigned to semantically similar images. \n\nThe clustering procedure starts with the mining of nearest neighbors in the space computed by the pretext task.  Each sample point and its found nearest neighbors is considered a “ground-truth” cluster. With this data, they learn a clustering function that will be able to assign each new sample to a cluster c.  \n\nThe weights learned from training with the pre-text tasks are used as initialization for their clustering training. \n\nIn addition, they employ a “fine-tuning” procedure using self-labeling. In cases where the learned clustering mapping assigns a high probability to a given cluster c given a new sample point X, it is assumed that this prediction (X, c) is correct, and is used as additional ‘ground-truth’ labels in the clustering loss. This additional self-supervised fine-tuning corrects for mistakes done in the first part of the clustering training.\n\nTheir architecture used in all experiments is standard ResNet backbone. The number of clusters here corresponds to the number of known classes.",
    "results": "  The obtained clustering accuracy is superior to other clustering state-of-the-art. This is an interesting method to keep in mind if there are no labels available at all for classification purposes."
  },
  "labels": [
    " unsupervised learning",
    " clustering",
    ""
  ]
}]