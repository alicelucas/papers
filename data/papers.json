[{
  "_id": {
    "$oid": "603e6c62a603d992335c9317"
  },
  "authors": "Carion, N., Massa, F., Synnaeve, G., Usunier, N., Kirillov, A., & Zagoruyko, S.",
  "date": "2020",
  "journal": "European Conference on Computer Vision",
  "title": "End-to-end object detection with transformers",
  "__v": 0,
  "sections": {
    "why": "In most traditional CNN-based methods for object detection, the workflow isn’t end-to-end and involves some amount of “preprocessing” or “postprocessing.” \n\nFor example there are a lot of works (including R-CNN-based methods) that are going to pose \nthe problem as first predicting a set of candidate boxes, and then classifying these boxes. These types of preprocessing are not ideal, because it makes the performance of the overall model very dependent on the engineering decisions made. \n\n“Postprocessing” often includes the use of NMS methods to remove duplicate bounding box predictions. \n\nIn this work, the object detection workflow is much simpler. That’s why their paper is called “end-to-end” object detection. That’s a big thing. \n",
    "what": "They choose to formulate the object detection problem as a set prediction problem. \n\nNote that they are not the first to approach the object detection problem as a set prediction problem. There exist methods where RNNs do that, but these methods are auto-regressive. This work here uses parallel decoding. (They make use of the recent work of parallel decoding with transformers)\n\nFurthermore, and that’s where there is the most novelty: they use an encoder-decoder transformer architecture with parallel decoding to model the inference. ",
    "how": "It’s a set prediction problem, so the target is going to be a set of size N, containing tuples of the (class, bounding box) of the instances in that image, and everything else in that set will be the “no object” label. This “no object” label can be seen as a way to pad the set. The architecture will be such that it outputs a set of that same size N. Again, the architecture is able to predict the “no object” label. The important thing to understand is that the target prediction is a set of N instances (most of them will correspond to “no object”), and the proposed prediction is a set of N instances as well.\n\nThe biggest challenge when formulating things as a set prediction problem is to match the prediction of the network to an instance in the target set. The way this is achieved here is with a “bipartite matching loss”. It uses the outputted classes and bounding box locations to try to match each prediction instance to another instance in the target set. Note that this results in a one-to-one matching, so we can be sure that multiple predicted bounding boxes will not be mapped to the same instance in the target set. That’s why NMS is not needed in this workflow. \n\nOnce the predictions are matched to the target instances, we use a Hungarian loss to actually train the neural network. This loss is very intuitive and similar to simple classification loss used in object detection. There is an extra novelty that they use a generalized IOU loss on the bounding box loss component, which helps making the loss a bit more scale invariant. \n\nThe user of a transformer architecture is the key distinguishing feature of their paper. Figure 2 in their paper shows the following:\n- An initial CNN, named “backbone” CNN is going to extract features from the initial image. There is going to be more channels and a smaller spatial extent. Then this feature map of size C x h x w is going to be flattened into a further compressed vector of size d x (hw). That’s the sequence that’s going to be fed at the input of the encoder. In some sense, each “time step” corresponds to a pixel in the image (with a feature vector representation of size d).\n- The encoder part of the transformer takes in this image sequence, in addition to positional encoding information. This is necessary because all information about position has been lost when we flattened the image and feed that to the encoder. The encoder then is going to output feature representations of those images. At this point, the encoder should have learned to focus on specific parts of the image to extract different instances.\n- The decoder part of the transformer is going to look at the extracted features from the encoder and output a set of bounding boxes. The decoder architecture also takes in “object queries”. It is necessary to input object queries to the decoder: to output N things, we need to input N things to it. These are learned during training. When combining information from feature maps from the object query, and feature maps from the encoder, then we can obtain bounding boxes.",
    "results": "The most interesting part of this paper is their section on visualizing the attention of the encoder and decoder networks.\n\nThey show that the encoder pays attention to the different instances in the image. Note that it does really well as separating close / overlapping instances. \n\nThe decoder clearly pays attention to the extremities of the object. It seems that it learns from the feature representation given by the encoder, and understands that extremities is what matters, and predicts a bounding box from that. \n\nAs a result of training, each “slot” (box) in the decoder becomes responsible for looking at a specific area of the image. This is shown in Figure 7. It seems like each slot is responsible for predicting a bounding box at a specific region of the image. \n\nIn terms of performance w.r.t RCNN and its different flavors, they show that they perform similarly (better for predicting large boxes, a bit worse for predicting small boxes). \n\nFinally, they show that they can easily apply their DETR model to the task of panoptic segmentation. All that is needed is a “mask head” (a CNN) to be appended to the features outputted by the decoder. The resulting network can be trained for panoptic segmentation (jointly, or not). They do very well. "
  }
},{
  "_id": {
    "$oid": "603e6c9ba603d992335c9318"
  },
  "authors": "Chen, T., Kornblith, S., Norouzi, M., & Hinton, G.",
  "date": "2020",
  "journal": "International conference on machine learning ",
  "title": "A simple framework for contrastive learning of visual representations.",
  "__v": 0,
  "sections": {
    "why": "The contrastive learning idea (of making representations of an image agree with each other) is not new; it dates back to a Becker & Hintor paper in 1992. But previously proposed contrastive learning requires specialized architecture or a memory bank. \nThis new framework removes the need for that. They spell out the necessary elements for having a successful simple contrastive learning procedure. ",
    "what": "SimCLR algorithm is composed of three steps: (1) transformation, (2) representation, (3) projection. The latter two steps are done by a neural network. The projection neural network is typically one or two linear (or non-linear) layer.\n",
    "how": "In this contrastive learning setup, an image is transformed in two different ways. These two resulting data points are called a “positive pair”. Each will be inputted to the same neural network (composed of a representation network and a projection head network) that will extract a feature vector for each of them. \n\nWe want the extracted representations of two transformed images to agree with each other.   The contrastive learning task will have a loss function that is going to maximize the “agreement” between these two feature vectors. \n\nWith these representations, we can then use these for linear classification. Or we can use the obtained network as an initial point for fine-tuning or transfer learning procedures.\n\nThe authors try out many different conditions, including different combinations of data augmentations, different architectures, and loss functions. \n\nThe end goal of experimenting with these different conditions is to determine which one leads to the best representations for a subsequent supervised learning task. They use a “linear evaluation protocol” which is going to train a classifier directly on these extracted representations, from the different settings. The better the final performance on the classification task, the better the representation is thought to be.",
    "results": "Learning representations in the unsupervised setting benefit from:\n- Composing augmentations, particularly the combination of random cropping and random color distortion. Unsupervised learning benefits more from this than supervised counterparts.\n- Augmentations can replace the design of complex architectures as previously done in contrastive learning.  For example, random cropping of  an image can easily provide adjacent viewers and global/local views for the object in that image. \n- Increasing depth and width. Unsupervised learning benefits more from this than supervised counterparts.\n- Adding a projection head improves the representation learned. If it’s a non-linear projection it’s better. However note that the representation used for the downstream task should be that of the layer before the projection head, not the one after (See Section 4.2). \n- The contrastive training loss that resulted in the best performance is the NT-Xent loss (Normalized, Temperature-scaled cross entropy loss). They find that the use of l2 normalization and temperature scaling is crucial for the resulting representation.\n- Training longer and larger batch sizes leads to better representations as well. That is because the network ends up seeing more negative examples, which facilitates convergence.\n- Compared with state-of-the-art (other methods for learning representations): they consistently outperform them for the linear evaluation mode and in the semi-supervised mode (in which the model is fine-tuned over few data points in ImageNet). "
  }
},{
  "_id": {
    "$oid": "603e6ce1a603d992335c9319"
  },
  "authors": "Bochkovskiy, A., Wang, C. Y., & Liao, H. Y. M.",
  "date": "2020",
  "journal": "arXiv preprint arXiv:2004.10934",
  "title": "Yolov4: Optimal speed and accuracy of object detection",
  "__v": 0,
  "sections": {
    "why": "The authors’s objective is to have an Object Detection model that works in the real world setting (for example for recommendation systems). Most works do not pay enough attention to the inference cost, which could in fact prevent these models to be implemented for real-world applications. \n\nMost researchers do not take that into account when developing new models, where accuracy is improved at the expense of inference cost.\n\nThe objective is to implement a model that works in real time, and only needs a conventional (normal) GPU for training and testing, and that works as well as the other, more costly methods. ",
    "what": "Their search for the perfect model for real-time prediction is done by looking in-depth into all of the existing architectural and training design features that have been introduced in the literature, and shown to have a positive effect on the accuracy of the model. \n\nThey define the architecture as composed of individual components: a backbone, a neck, and a head. How each of these components can have a different number and different types of layers. \n\nThe training design mechanisms are classified into two different types of groups, the “Bag of freebies,” and the “Bag of specials.”  “Bag of freebies” lead to better accuracy with no increase in inference cost. These include data augmentation, special example mining, or better loss functions. “Bag of specials” leads to better accuracy, but may come with an inference cost increase. These include special attention modules, feature integration mechanisms, or the design of activation functions. \n",
    "how": "They are going to (1) study the different existing architectural components, and (2) experiment with the different tools obtained from bag of freebies and bag of specials. \n\nIn addition to listing all of the BoF/BoS and experimenting with them, they (1) introduce new data augmentation methods and and (2) they change existing design methods such that they lead to more efficient training and detection (See Section 3.3). The resulting architecture and BoF/BoS selections are shown in the left column of Page 7. \n\nAll of their different combinations are tested by using ImageNet’s (ILSVRC) dataset and the COCO test dataset. For all of their experiments, they only use one GPU for training. \n",
    "results": "They choose CSPDarkNet53 as their backbone. For the neck, they find that SPP (Spatial Pyramid Pooling) blocks and a PANet aggregation neck works best. Finally, their head, responsible for the detection, is chosen to be YOLOv3. Their resulting model is named YOLOv4.  \n\nTheir resulting YOLOv4 becomes the fastest and most accurate detector proposed in the literature.\n\nNote that they provide a link to their GitHub repo which includes a lot of documentation. \n"
  }
},{
  "_id": {
    "$oid": "603e6d51a603d992335c931a"
  },
  "authors": "Wu, S., Rupprecht, C., & Vedaldi, A.",
  "date": "2020",
  "journal": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition",
  "title": "Unsupervised learning of probably symmetric deformable 3d objects from images in the wild",
  "__v": 0,
  "sections": {
    "why": "There is a vast literature on the topic of 3D reconstruction, which includes non-learning-based and learning-based methods. In the learning-based approaches, it’s often the case that the supervision signal uses 3D ground truth, or videos, or 2D keypoint annotations, etc. \n\nThis work never uses 3D labels (or related labels) as part of its supervision. It is completely unsupervised. It uses a single-view input image as its input (whereas other works utilize multiple views of that image to reconstruct the 3D). \n",
    "what": "The learning problem is reconstructive. Given an input image, the network’s task is going to reconstruct the input image. Note that this network is faced with an inverse problem: given the observed input image, the objective is to recover the underlying parameters that make its 3D structure. \n\nAn additional guidance principle in their work is that of symmetry. They assume that many object categories have, at least in principle, a symmetric structure. This assumption will be incorporated in the overall loss function. \n",
    "how": "The input is the 2D image, and the output are the four parameters (depth, albedo, viewpoint and lighting). This corresponds to a 3D internal representation, from which a 3D image can be rendered.\n\nThe symmetry is learned via two mechanisms:\n- Once we have the outputs (albedo, light, depth, viewpoint) of the individual neural networks, It is possible to render a symmetric reconstruction by flipping, across the vertical axis, the albedo and depth maps. Symmetry is then learned by enforcing that this reconstructed symmetric image, once projected back to 2D, is equal to the input image. \n- Also, they augment the model to reason about potential lack of symmetry in objects, by having it predict a probability map of whether a given pixel has a symmetric counterpart in the image. This probability map is used when computing the loss term.\n\nThe learning objective is reconstructive: the model is trained to reconstruct the input image. It learns to decompose the input image into a (depth, albedo, viewpoint and lighting) parameters, and input these to a renderer that outputs a 3D image. The loss component is going to apply a projection to the resulting 3D image and enforce that it matches the input image. \nThere are two loss terms in the overall loss: the reconstructive loss that enforces that the image from the predicted parameters is equal to the input image, and a loss that enforces that the image from the symmetrically predicted parameters is equal to the input image. \n",
    "results": "\nThey test their methods on human face datasets, cat face dataset, and synthetic cars datasets.\nAs a result of training, the resulting autoencoder internally decomposes the image into albedo, depth, illumination and viewpoint, without direct supervision for any of these factors. \n\nTheir evaluation metrics use the ground truth depth maps that they have in their dataset. Given the depth maps predicted by their model, they can compute the “scale-invariant depth error” (SIDE) metric as an evaluation metric. In addition, they use the “Mean Angle Deviation”, again computed from the depth maps.\n\nThey can also do a qualitative comparison with SOTA by just comparing the quality of the 3D reconstructions, see for example Figure 7.\n\nVia ablation studies, they show that symmetry and illumination are crucial cues to have the model converge to a good reconstruction. \n\nTheir model even outperforms a 3D reconstruction method that uses 2D keypoint supervision as additional input (whereas they only use the single viewpoint input image). "
  }
}]