(this.webpackJsonppapers=this.webpackJsonppapers||[]).push([[0],{160:function(e,t,a){},161:function(e,t,a){},210:function(e){e.exports=JSON.parse('{"Items":[{"date":{"S":"2017"},"journal":{"S":"NeurIPS 2017"},"labels":{"L":[{"S":"attention"},{"S":" transformers"}]},"sections":{"M":{"why":{"S":"This is the paper that introduced Transformers to the literature. Since their introduction, transformers have become a popular component of most architectures in sequence modeling or translation tasks (for example both BERT and GPTs use transformers). Transformers are now making their way into image-based tasks. \\n"},"how":{"S":"A transformer model is composed of an encoder and a decoder stack, each consisting of stacked layers that are themselves composed of two sub-layers: a multi-head attention layer followed by a fully connected feedforward layer.  The decoder layers have an extra multi-head attention layer that is applied to the output of the encoder stack.\\n\\nMulti-head attention layers consists of multiple dot-product attention layer: values are scaled by a certain weight, which is computed by a compatibility function of the query with the corresponding key. The multi-head attention layer projects the queries, keys, and values h times with different, learned linear projections. On each of these projected versions, the attention function is performed in parallel (See figure 2). The results are concatenated and once again projected, resulting in final values outputted by the multi-head attention layer. \\n\\nWhere do the queries, keys, and values come from? It depends whether we are using self-attention layers or \u201cencoder-decoder\u201d attention layers. In self-attention layers, of all the keys, values, and queries come from the previous layer of the encoder or decoder stack. In encoder-decoder attention layers, the queries come from the previous decoder layer, and the keys and values come from the previous encoder layer. \\nThe encoder part of the transformer uses self-attention layers only, but the decoder part of the transformer uses both self-attention layers and encoder-decoder attention layers (See Figure 1). \\n\\nBecause their model does not use recurrence or convolution, they need to keep track of positional embeddings. These are added to the input embeddings of the encoder and decoder stacks. \\n"},"what":{"S":"Prior to this paper, there did exist papers that used attention with encoder and decoders as part of their neural network architecture, but the difference is that these papers also relied on recurrent or convolutional layers in addition to the used attention layers. \\n\\nThe novelty of this particular paper is that their architecture is based on Transformers only, relying entirely on an attention mechanism between the input and output. With that, they achieve new state-of-the-art on the translation tasks."},"results":{"S":"Using attention layers have multiple advantage. It reduces total computational complexity per layer, and also allows more computation to be parallelized. Finally, it is more easy to learn long-range dependencies in the input. \\n\\nTable 2 shows their results on several translation tasks. Their \u201cbig\u201d Transformer model outperforms the other models on the English-to-French and English-to-German translation tasks. They also need perform less FLOPS than the other models."}}},"id":{"S":"7d2b7cfd-e588-449f-99ee-5cc2552d721c"},"title":{"S":"Attention is all you need"},"authors":{"S":"Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ... & Polosukhin, I."}},{"date":{"S":"2020"},"journal":{"S":"arXiv preprint arXiv:2004.10934"},"labels":{"L":[{"S":"Object detection"},{"S":"Image classification"}]},"sections":{"M":{"how":{"S":"They are going to (1) study the different existing architectural components, and (2) experiment with the different tools obtained from bag of freebies and bag of specials. \\n\\nIn addition to listing all of the BoF/BoS and experimenting with them, they (1) introduce new data augmentation methods and and (2) they change existing design methods such that they lead to more efficient training and detection (See Section 3.3). The resulting architecture and BoF/BoS selections are shown in the left column of Page 7. \\n\\nAll of their different combinations are tested by using ImageNet\u2019s (ILSVRC) dataset and the COCO test dataset. For all of their experiments, they only use one GPU for training.\\n"},"why":{"S":"The authors\u2019s objective is to have an Object Detection model that works in the real world setting (for example for recommendation systems). Most works do not pay enough attention to the inference cost, which could in fact prevent these models to be implemented for real-world applications. \\n\\nMost researchers do not take that into account when developing new models, where accuracy is improved at the expense of inference cost.\\n\\nThe objective is to implement a model that works in real time, and only needs a conventional (normal) GPU for training and testing, and that works as well as the other, more costly methods."},"what":{"S":"Their search for the perfect model for real-time prediction is done by looking in-depth into all of the existing architectural and training design features that have been introduced in the literature, and shown to have a positive effect on the accuracy of the model. \\n\\nThey define the architecture as composed of individual components: a backbone, a neck, and a head. How each of these components can have a different number and different types of layers. \\n\\nThe training design mechanisms are classified into two different types of groups, the \u201cBag of freebies,\u201d and the \u201cBag of specials.\u201d  \u201cBag of freebies\u201d lead to better accuracy with no increase in inference cost. These include data augmentation, special example mining, or better loss functions. \u201cBag of specials\u201d leads to better accuracy, but may come with an inference cost increase. These include special attention modules, feature integration mechanisms, or the design of activation functions. \\n"},"results":{"S":"They choose CSPDarkNet53 as their backbone. For the neck, they find that SPP (Spatial Pyramid Pooling) blocks and a PANet aggregation neck works best. Finally, their head, responsible for the detection, is chosen to be YOLOv3. Their resulting model is named YOLOv4.  \\n\\nTheir resulting YOLOv4 becomes the fastest and most accurate detector proposed in the literature.\\n\\nNote that they provide a link to their GitHub repo which includes a lot of documentation. \\n"}}},"id":{"S":"32cf43d8-6a6e-4998-8630-7670f56a50b5"},"title":{"S":"Yolov4: Optimal speed and accuracy of object detection"},"authors":{"S":"Bochkovskiy, A., Wang, C. Y., & Liao, H. Y. M."}},{"date":{"S":"2021"},"journal":{"S":"arXiv preprint arXiv:2101.11605"},"labels":{"L":[{"S":"Transformers"},{"S":"Object detection"},{"S":"Attention"}]},"sections":{"M":{"why":{"S":"They propose a new neural network architecture for the backbone that can be used for multiple visual recognition tasks (classification, object detection, segmentation). Note that for the case of object detection, this is a proposed architecture for the backbone, and that it is still necessary to use a separate detection framework (e.g. DETR or RCNN) on top of it. Another application would be for instance segmentation: we can replace the state-of-the-art FPN backbone with their proposed BotNet backbone, and put a Mask-RCNN on top of it."},"how":{"S":"The main argument is that many vision tasks such as object detection or instance segmentation need to be looking at the whole context of the image. Convolutional neural networks can do that by stacking multiple convolutional layers on top of one another. This generally works, but the authors argue that an explicit mechanism for modeling long range dependencies would work better. The use of \u201cMulti-Head Self-Attention\u201d (MHSA) layers, used in Tranformer encoders, is an appropriate choice for this explicit mechanism.\\n\\nTheir architecture is actually the traditional ResNet architecture, but its final 3 bottleneck layers are replaced with three special \u201cbottleneck transformer\u201d. The rest of the ResNet architecture is untouched. A bottleneck transformer consists of (1) a convolution layer that downsamples the feature maps, (2) the self-attention transformer process (the MHSA layer) that aggregates information from the feature maps.\\n\\nNote that they also input positional encodings to their architecture, as many Transformer-based architectures do for vision tasks. They find that using relative positional encoding provide better performance than using absolute positional encoding. "},"what":{"S":"Their contribution is in the proposed architecture of the backbone used for object detection, named BotNET. While they are not the first to propose a model that uses both convolution with transformer blocks (e.g., DETR and VideoBERT), they are the first to integrate these transformer blocks within the CNN backbone itself as opposed to stacking transformer blocks separately on top of the CNN backbone. "},"results":{"S":"They replace the traditional backbone ResNet with their BoTNet and attach the mask R CNN object detector on top of it. On the COCO instance segmentation dataset, theres seems to be a slight improvement over using the ResNet backbone. \\n\\nNote that BotNet does not provide significant gains when trained on the ImageNet classification task. This is because ImageNet has much smaller images, typically 224 x 224, and by the time we get to the transformer block, the images have been downsampled too much to learn anything useful. A way to go about this is to use a stride of 1 in the MHSA layers \u2014 doing so does result in an increase of performance.  "}}},"id":{"S":"a876f6a6-8934-44dc-857f-3937b3922206"},"authors":{"S":"Srinivas, A., Lin, T. Y., Parmar, N., Shlens, J., Abbeel, P., & Vaswani, A."},"title":{"S":"Bottleneck Transformers for Visual Recognition"}},{"date":{"S":"2017"},"journal":{"S":"arXiv preprint arXiv:1704.04861"},"labels":{"L":[{"S":"Mobile application"},{"S":" Depthwise Separable Convolutions"},{"S":" Classification"},{"S":" Object Detection"}]},"sections":{"M":{"why":{"S":"A lot of the works in the literature focus on obtaining a model that is performant in terms of accuracy but rarely considers the size of the model and speed of inference. This is problematic for those who wish to deploy neural networks as part of a mobile application. \\n\\nThis work proposes an architecture that is lightweight enough to be integrated in mobile apps, while still being competitive with the other state-of-the-art CNNs on ImageNet and other datasets. "},"how":{"S":"The core layers of their EfficientNet network uses a convolutional layer named \u201cdepthwise separable convolutions\u201d (these are not new, they have been introduced in the literature before, and used in the Inception model). Depthwise separable convolutions consist of (1) \u201cdepthwise convolutional filters\u201d that are only going to be applied along the depth dimension and not the width and height, and followed by (2) 1 x 1 convolution filters called \u201cpointwise convolutions\u201d that are going to be applied on the result of the depthwise convolutional filters to output the new feature maps for the next layer. \\n\\nBy using depthwise separable convolutions instead of conventional convolution layers, we remove the interaction between the number of output channels and the size D x D of the kernel. As a result, the authors calculate that we obtain a reduction in computation of 1 / N + 1 / D^2, where N is the number of outputs channels for a layer and D is the size of the kernel. In practice this means that we get a network that requires almost 10 times less computation and experiences just a small reduction in accuracy.  \\n\\nThey note that because their model is smaller than other ResNet models and hence less prone to overfitting, they use very little regularization and data augmentation in their experiments. Furthermore, they do not apply L2 regularization on the depthwise separable convolutions. \\n\\nIn addition to introducing a lighter architecture, they also introduce a parameter alpha, called the width parameter, that allows the developer to design smaller models if the requirement specify so. The width parameters alpha determines the number of input and output channels at each layer: the number of input channels is alpha * M and the number of output channels is alpha * N, where alpha is between 0 and 1. Alpha = 1 is the baseline MobileNet model. \\n\\nSimilarly, they introduce a resolution multiplier rho, which serves the role of reducing the computational cost of the neural network by scaling the image down to a smaller resolution. This multiplier is applied to the input image only which then has the effect of decreasing the spatial extents of each feature maps throughout the network. "},"what":{"S":"Most works that aim to introduce a new efficient architecture usually do so by either (1) using compression/shrinking/factorizing methods on large pretrained networks or (2) by training on small networks architecture directly.\\n\\nThis work belongs to the latter approach, where they propose to use efficient convolutional layers to replace conventional convolutional layers to make their MobileNet model lighter. In addition,  they introduce two hyperparameters that allow the model developer to choose the network model that matches the resource restrictions for their particular application. In some sense, using these two hyperparameters as control parameters, the model is \u201ccustomizable\u201d depending on the resources available at hand and will choose the best trade-off between latency, size, and accuracy. "},"results":{"S":"They show the results of applying their baseline MobileNet on ImageNet and compare the effects for different width and resolution parameters. Tables 4 -7 are very informative. They show that there is no large loss of accuracy when switching from regular convolutions to depthwise separable convolutions, or when using smaller width and resolution parameters, whereas there is a large decrease in needed computational resources. \\n\\nThey also compare their MobileNet model with different state-of-the-art (GoogleNet, VGG, SqeezeNet, AlexNet) of the time, and find that their model is competitive, while having very little mult-adds operations and parameters compared to the other models.\\n\\nIn addition, they train their model on the COCO object detection task, setting MobileNet as the backbone model of their object detection model (using a Faster-RCNN and a SSD object detection framework). They compare  this approach with the use of VGG or Inception V2 as alternative backbone models, and find that they perform competitively as well. They are close to Inveption but worse than VGG. It is important to emphasize that the reductions in mult-adds and parameters is huge: see table 13.\\n\\nNote: In 2019, a MobileNetV2 paper was published, which addresses the case of object detection with smaller models in more detail (a framework called SSDLite), and the case of semantic segmentation (a framework called Mobile DeepLabv3). This model is implemented in Tensorflow."}}},"id":{"S":"c5649e13-8855-4fbb-8e9a-eff66837574f"},"title":{"S":"Mobilenets: Efficient convolutional neural networks for mobile vision applications"},"authors":{"S":"Howard, A. G., Zhu, M., Chen, B., Kalenichenko, D., Wang, W., Weyand, T., ... & Adam, H. "}},{"date":{"S":"2019"},"journal":{"S":"arXiv preprint arXiv:1912.11370,\xa06(2), 8."},"labels":{"L":[{"S":"Supervised pre-training"},{"S":" transfer learning"},{"S":" fine-tuning"}]},"sections":{"M":{"why":{"S":"They study transfer learning, where we do supervised pre-training and fine-tune on a task. Doing such pre-training is usually very helpful because it results in (1) Label efficiency and (2) simplified hyperparameter tuning."},"how":{"S":"They train three different Big Transfer architectures, each of them being of different scale, and trained on three different datasets of different sizes: the ILSVRC-2012 (1.3 M images), the ImageNet-21k dataset (14 M images) and the JFT dataset (300M+) images. Each of these models are called BiT-S, BiT-M, and BiT-L.\\n\\nThey show that Batch Normalization in the BiT architecture is not recommended and can be detrimental for small batch size. Instead, their BiT architecture is a ResNet where there Batch Normalization is replaced with Group Normalization layers and Weight Standardization layers, which they found results in improvement in the fine-tuning tasks.\\n\\nFor a given fine-tuning task, they focus on only three hyper-parameters: the training schedule length, what resolution to apply on the images during fine-tuning, and whether to use MixUp regularization. The choice of this parameter is done as a simple function of the tasks\u2019s intrinsic image resolution and the number of data points for the task. This protocol is called BiT-HyperRule. See the first paragraph on Page 6 for details. "},"what":{"S":"This paper provides a \u201crecipe\u201d for optimal transfer learning, a recipe that they call \u201cBig Transfer.\u201d They find that using this recipe consistently attains excellent performance on the fine-tuned task. Part of their recipe is to propose a simple heuristic for setting proper hyper-parameter tuning, which they find works across fine-tuning applications. "},"results":{"S":"They fine-tune their pre-trained model on different tasks, some of them which only have 1 example per class, some of them have over 1M total example (e.g. ImageNet). For all of those tasks, they get excellent results. \\n\\nThey show that pre-training on their JFT-300M dataset works better than pre-training on the ImageNet-21K dataset.\\n\\nAn interesting finding that they have is that they find that weight decay or dropout is not needed when fine-tuning on the downstream task. This is a bit surprising given how large the BiT models are and how small the fine-tuning datasets are \u2014 but they argue that good regularization can be obtained by simplify modifying the schedule length of their fine-tuning (this is one of their hyper-parameters)."}}},"id":{"S":"0ec5ab68-a2f0-4312-99e1-3f01c9ce7975"},"title":{"S":"Big transfer (bit): General visual representation learning"},"authors":{"S":"Kolesnikov, A., Beyer, L., Zhai, X., Puigcerver, J., Yung, J., Gelly, S., & Houlsby, N.\xa0"}},{"date":{"S":" 2018"},"journal":{"S":" In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"},"labels":{"L":[{"S":" Unsupervised learning"},{"S":" Weakly supervised learning"},{"NULL":true}]},"sections":{"M":{"how":{"S":" The authors propose to identify relationships between treatments by looking at the similarity of the morphologies of cells resulting from treating cells with those treatments. If two treatments result in a similar morphology, then it can be concluded that these treatments are related. \\n\\nThe authors propose a new way to learn a morphology representation. The input to the CNN is the image of a single cell. The output is the treatment label. Essentially, the CNN is trained to recognize the features produced by each treatment. This auxiliary task is what allows us to then use the learned representation for the real task, which is to associate treatments with each other. \\n\\nThe CNN architecture corresponds to the ResNet18 architecture. They note that they did not find significant difference when testing different ResNet or VGG models. \\n\\nIt is important to note that the labels are very noisy: treatments may not not even have any effect on the morphology of images, or different treatments may even have the same effect on the morphology. They regularize the training by using RNN-based regularization, where a chain of GRUs are going to look at a group of images with the same treatment label instead of a single image. A second regularization method they use is called \u201cmixup\u201d, where they artificially create new labels by creating linear combinations of those. The output label is the linear combination of the original labels. \\n"},"why":{"S":" This work proposes a way to learn representations of single cells in microscopy image. Having such quantitative representations is important for many applications, for example in drug discovery pipelines. \\n\\nFor example, quickly assessing the morphological changes of cells due to a drug, by computing its representation and comparing it with representations of other images (e.g. healthy cells, or cells that also have been undergoing treatment) can accelerate drug discovery pipelines or make them more accurate. \\n\\nWhat needs to be clear here is that efficacy of treatment is obtained by looking at how the cells look after being treated with the treatment."},"what":{"S":" Most previous papers that tackled microscopy image problems belong to the supervised setting. This paper is one of the first that propose a weakly supervised way to learn representations of cells. Because the main goal of morphological profiling is to compare treatments with each other, \u201cground-truth\u201d data in profiling experiments usually corresponds to known high level associations between treatments. \\n\\nWe usually do not have such ground-truth data since we don\u2019t know in advance how different treatments relate to one another. However, we do have a ground-truth signal that we can use as an auxiliary task: we know that the same treatment applied to different cells should result in the cells having similar morphologies at the end of the day. \\n\\nFurthermore, the authors recognize that the data is noisy and prone to bias issues (e.g. due to batch variations or imaging artifacts) and therefore explore in detail different regularization schemes. \\n"},"results":{"S":" To visualize the representations, they do tSNE visuallization of these representations. They show that distinct clusters (each corresponding to a specific drug) are shown with these embeddings, which is not the case for the features obtained from CellProfiler or Inception. \\n\\nFurthermore, they try to do NSC classification (not-same-compound) classification using these features. They show that features from intermediate layers work better than later layers of ResNet. This is consistent with what I\u2019ve been reading in the literature on unsupervised learning (taking the later layers is rarely a good idea).  "}}},"id":{"S":"1b0ae710-b400-484d-911b-ff57c040e065"},"title":{"S":" Weakly supervised learning of single-cell feature embeddings"},"authors":{"S":" Caicedo, J. C., McQuin, C., Goodman, A., Singh, S., & Carpenter, A. E."}},{"date":{"S":"2015"},"journal":{"S":"In\xa0ICML deep learning workshop\xa0(Vol. 2)"},"labels":{"L":[{"S":"One-shot learning"},{"S":" Siamese networks"}]},"sections":{"M":{"why":{"S":"This work proposes a new approach for solving the one-shot classification problem. The one-shot classification problem corresponds to the case scenario where we have a trained model for a certain number of classes, and at test time we want it to classify a new example for which we only have one ground-truth sample point. \\n\\nNote that an important thing to understand about one-shot or multiple-shot recognition is that the weights of the network are not modified at test time \u2014 there has to be no fine-tuning. The objective is to obtain a good classification with the weights that we have from training. Typically the feature maps or the embeddings is what is used to solve the problem. "},"how":{"S":"The objective of that training task is to learn very discriminative features of a given instance and then use these features to leverage the one-shot classification problem.\\n\\nIn a Siamese network, two copies of the same neural network architecture will each take in a different input. They each compute a feature vector for their respective input, which are then joined together and fed to a final layer that determines whether the two inputs belong to the same class by assigning a  probability. The objective function is a categorical cross entropy problem.\\n\\nAt evaluation time, the input to one of the Siamese network is the new example image, and the input to the other Siamese network is a representative image from a known given class. They do that for each class in the dataset, obtaining a probability for each combination of input, where each combination has a different known class associated with the representative image. The combination of inputs that results with the highest probability score is used to assign the class to the test example. \\n\\nFor this to work, it is important that the network has been trained on a set of images that are diverse, to encourage variance amongst the learned features. "},"what":{"S":"When this paper was published (2015), there was not much research on one shot recognition. They are the first to propose a solution that uses Convolutional Neural Networks. "},"results":{"S":"The Omniglot dataset contains pictures of characters from ~50 different alphabets. The task of the Siamese network is to determine whether two characters belong to the same character class. They keep 10 alphabets for evaluation and only train on 40 alphabets. \\n\\nOn that task, they surpass all methods except for the Hierarchical Bayesian Program Learning method. This method, however, utilizes prior knowledge on strokes whereas this Siamese CNN approach does not and learns all knowledge from the dataset itself. \\n\\nThis way of learning feature spaces reminds me of contrastive learning. While they do not use the same loss (here they use binary cross-entropy for classification task but contrastive loss will try to maximize agreement between the two feature vectors), it is a similar concept. Another difference is that this paper utilizes a relatively shallow CNN, whereas the recent works on contrastive learning are performed on deep ResNet models."}}},"id":{"S":"cb5d8aac-9a59-43be-ad2e-4521629b3d18"},"title":{"S":"Siamese neural networks for one-shot image recognition"},"authors":{"S":"Koch, G., Zemel, R., & Salakhutdinov, R."}},{"date":{"S":"2020"},"journal":{"S":"Advances in Neural Information Processing Systems,\xa033"},"labels":{"L":[{"S":"Unsupervised learning"},{"S":" self-training"},{"S":" pre-training"}]},"sections":{"M":{"why":{"S":"It is often the case that prior to training a model on a object detection or segmentation task, the model was pre-trained on the ImageNet classification task. \\n\\nThis work describes cases in which such pre-training on the ImageNet dataset is not suitable and hurts the final accuracy, and self-training should be favored. \\n\\nIn addition, they provide two new state-of-the-art models for object detection and segmentation, obtained as a result of using their own self-training setup. "},"how":{"S":"Their experiments address two use cases: improving accuracy when training on the COCO Object Detection dataset and when training on the PASCAL segmentation dataset. They investigate with different ways of doing data augmentation (some stronger than others) and different labeled COCO dataset size as control factors in their experiments.  \\n\\nThey detail the cases where pre-training should not be used, and self-training should be favored."},"what":{"S":"Pre-training refers to training a random model on the ImageNet classification dataset. \\n\\nSelf-training refers to (1) training a teacher model on the given task (e.g. object detection with the COCO dataset), (2) using the teacher model to generate labels (e.g. bounding boxes) on an unlabeled dataset (e.g. ImageNet) and (3) combining the COCO dataset and the newly labeled dataset to train a new student model. \\n\\nThese authors study in the detail whether and when pre-training and self-training should be used."},"results":{"S":"As they increase the strength of data augmentation or increase the amount of labeled data in the COCO dataset, the effect of pre-training diminishes. With very strong data augmentation, pre-training can actually hurt the accuracy. They hence recommend to not use pre-training if there is a lot of data or if employing a lot of data augmentation.\\n\\nThey also investigate the value of unsupervised learning to obtain a pre-trained model, for example using the SimCLR algorithm. They find that similarly to ImageNet pre-training, using SimCLR can actually hurt the COCO detection performance in the large data / strong augmentation regime. \\n\\nOn the other hands, self-training interacts well with data augmentation, as data augmentation is found to usually help self-training. \\n\\nThey find that doing self-training on top of pre-training is always helpful. \\n\\nA personal note: I think it is a bit obvious that if I already have a very large labeled dataset for a given task, it might not make sense to use pre-training anyways. Instead of pre-training on an auxiliary task (e.g. classification or contrastive learning on ImageNet), I might as well train my model directly on the task for which I want it to perform. Self-training employs that approach. The authors do emphasize that in the case of a small amount of labeled dataset, pre-training is a good idea. They also suggest to use self-training on top of that. "}}},"id":{"S":"2f3cd1ae-36e4-4767-9e6c-83671458d13e"},"title":{"S":"Rethinking pre-training and self-training"},"authors":{"S":"Zoph, B., Ghiasi, G., Lin, T. Y., Cui, Y., Liu, H., Cubuk, E. D., & Le, Q. V.\xa0"}},{"date":{"S":"2020"},"journal":{"S":"arXiv preprint arXiv:2005.10821"},"labels":{"L":[{"S":"Semantic segmentation"},{"S":"Attention"}]},"sections":{"M":{"why":{"S":"This work proposes a new architecture for semantic segmentation which provides state-of-the-art IOU on the Cityscape and Mapillary vistas segmentation datasets. \\n\\nThey address the particular problem that CNNs for semantic segmentation typically combine different predictions at different scales (think FPN network), but the current ways to combine these have been either with average pooling or max pooling \u2014 both of which may not be appropriate and may not lead to the best final segmentation results. For example, a poor prediction will have a negative impact on the overall segmentation result when using it in average pooling. Max pooling, on the other hand, will ignore predictions at some scales that should have been given more weights to improve the final prediction. \\n\\nFurthermore, they present a new auto-labelling strategy that auto-labels unlabeled images and results in improved IOU performance. I won\u2019t dive into this auto-labeling procedure in this summary, but it\u2019s something to keep in mind to look back into it later."},"how":{"S":"Traditionally, using attention on multiple scales means pre-defining the sets of scales and weighing of these with their attention map. This paper however proposes a hierarchical method, where a relative attention mask is learned between adjacent scales. During training, the network only sees a pair of scales, where one prediction is obtained from scale 1 and other prediction is obtained from scale 0.5 of that. This means that during inference, predictions from multiple scales can be obtained hierarchically as shown the bottom right of Figure 2. For each scale, we look at half of that scale and combine the results. We do this iteratively until we have combined all scales. \\n\\nTheir architecture is composed of a ResNet-50 backbone, a semantic head, and an attention head. They find that using the HRNet-OCR as the backbone can produce better results. \\n"},"what":{"S":"Instead of using average pooling or max pooling, the authors choose to combine segmentation predictions at different scales using an attention mechanism in their network. Note that using attention for scale combination is NOT the novelty of the paper: there already exists multiple papers that have used attention to combine predictions at multiple scales, as cited in their Related Works section. However, these papers have the limitation that they define a fixed set of scales to look at during training, and this limits the scales that been used during inference. The authors of this paper however, propose an attention mechanism that is agnostic to the number of scales during inference time. Their mechanism also allows them to visualize the importance of each scale during a given prediction. \\n"},"results":{"S":"Clearly the main advantage of their method is the flexibility at inference time in terms of the allowed scales. Furthermore, this approach allows them to be more memory efficient and faster training. \\n\\nTheir resulting IOUs on Cityscapes and Mapillary surpass the current state-of-the-art results."}}},"id":{"S":"e7e78f23-76db-4450-a22d-fea73b02cc38"},"authors":{"S":"Tao, A., Sapra, K., & Catanzaro, B."},"title":{"S":"Hierarchical multi-scale attention for semantic segmentation"}},{"date":{"S":"2018"},"journal":{"S":"International Conference on Medical Image Computing and Computer-Assisted Intervention (pp. 265-273). Springer, Cham."},"labels":{"L":[{"S":"Object detection"},{"S":"Cell segmentation"},{"S":"Bioimage analysis"},{"NULL":true}]},"sections":{"M":{"how":{"S":"Stardist\u2019s architecture is a light-weight neural network based on U-Net. \\n\\nThe task of the network is to predict for each pixel: (1) the distance between that pixel and the nearest background pixel, which can be seen as a probability of belonging to an instance, and (2) the distance to the object boundary along each of the n predefined radial directions. \\n\\nA binary cross-entropy loss function is used to predict the object probabilities, and a MAE loss for predicting the radial distances. \\n\\nThey apply NMS to only retain polygons in a certain region with the highest object probabilities. They only consider polygons associated with pixels above a certain object probability threshold. \\n"},"why":{"S":"Accurate cell and nuclei segmentation is an important and crucial step in research. Current object detection SOTA methods, such as Mask-RCNN, do not adapt well to shapes such as those of nuclei and cells."},"what":{"S":"In object detection, a typical top-down approach is to first predict regions of interest and then classify the objects in these found regions of interest. This is different from the bottom-up approach, where the image is first classified with a semantic map, and then different instances are extracted. \\n\\nThis paper employs the top-down approach. It argues that the typically used bounding boxes are not appropriate shapes to use when segmenting cells or nuclei. The post-processing step NMS will not work well if the underlying objects are not accurately represented by these bounding boxes. Instead, they argue that star-convex polygons is the more appropriate shape to use. "},"results":{"S":"They compare their results with a UNet trained for a simple semantic classification task. They test on synthetic datasets of cells and nuclei, and on nuclei dataset from DSB 2018. On the real dataset of DSB 2018, StarDist outperforms all methods. StarDist does particularly well at not merging cells or not missing cells. This is supposedly due to the use of a star-convex polygon which creates less problems when using NMS.\\n\\nI wish they had provided a comparison of using their method with the same method, but with the only difference being the prediction of a bounding box instead of a star polygon shape. It\u2019s difficult to know here what is the effect of predicting star-convex shapes. They do compare with Mask-RCNN, but they use the open-source implementation of it, so the comparison may not be valid.\\n\\nTwo of their test datasets are synthetic, which is a bit of a disappointment for seeing the performance on cell data. A real cell test dataset should be used. "}}},"id":{"S":"78f7bd5c-b4f0-4d4f-b129-b927d785dcb4"},"title":{"S":"Cell detection with star-convex polygons"},"authors":{"S":"Schmidt, U., Weigert, M., Broaddus, C., & Myers, G."}},{"date":{"S":"2019"},"journal":{"S":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 6399-6408)."},"labels":{"L":[{"S":"Feature Pyramid Networks"},{"S":"Panoptic segmentation"},{"S":"Dilated convolutions"},{"S":"Multi-task learning"}]},"sections":{"M":{"why":{"S":"At the time of publication, the SoTA for semantic segmentation is a FCN with dilated convolutions (also called atrous convolutions), and for instance segmentation it is a Mask-RCNN with a Feature Pyramid Network (FPN) backbone. \\n\\nThis work addresses the task of panoptic segmentation, which is the joint task of doing instance segmentation and semantic segmentation. "},"how":{"S":"The proposal is to start with the Mask R-CNN with FPN backbone (SoTA in instance segmentation). On top of these FPN features, they append an instance segmentation branch and a semantic segmentation branch. They make no change at all to the FPN backbone. The changes needed to get the semantic segmentation branch are minimal. \\n\\nEach of these branches share the same Feature Pyramid Network (FPN) backbone. Their resulting model is named Panoptic FPN. \\n\\nThe FPN backbone utilizes features at different resolutions. FPN generates a pyramid of features, scaling from 1/32 to 1/4 resolutions. It has the particularity that there are the same number of channel dimensions at each level. \\n\\nIn the instance segmentation branch, we attach a region-based object detector like faster RCNN on each of its pyramid level, to obtain candidate bounding boxes. This is made possible with the fact that that FPN backbone has the same number of channels at each level. Then by applying a FCN layer on the resulting detected regions for binary segmentation, a segmentation map for each instance is obtained (This is essentially the approach taken by mask-RCNN). \\n\\nThe semantic branch is going to take in the FPN features and upsample them and transform them in several ways such that in the end we are left with several feature maps corresponding to the 1/4 resolutions. A final 1x1 convolution, upsampling, and softmax layer is generated for each pixel, providing us with the segmentation map. \\n\\nThe output of the Panoptic FPN is going to have (1) a single class label and (2) an instance id for each pixel in the image.\\n\\nTraining these two branches jointly is tricky: in this paper, they provide the necessary information regarding the loss balances, constructing mini-batches, adjusting learning rate schedules, and performing data augmentation. "},"what":{"S":"In the recent COCO and Mapillary Recognition competition on panoptic segmentation, every competitive entry ended up using a separate network for each of the tasks, with no shared computation. The previous literature that has tried to unify instance and semantic segmentation models have resulted in loss of performance in either of those tasks.\\n\\nWe are thus in need of a single model that can do both tasks simultaneously, but still perform competitively with as SoTA on each of those tasks. \\n\\nThis work therefore unifies these methods at the architectural level, by designing a layer which will be shared across networks. Their resulting model is a simple architecture that matches accuracy with SoTA for both tasks. The network simultaneously generates region-based outputs and dense-pixel outputs."},"results":{"S":"They evaluate their models on COCO and Cityscapes. They perform very well on instance segmentation, since the Mask RCNN + instance segmentation branch is essentially what SoTA does already. But they also do perform very well on semantic segmentation, as it is competitive as SoTA performing dilation based FCNs (such as DeepLabV3). This is observed when the network is trained for solving the two tasks independently, and when trained for solving the two tasks at once (panoptic segmentation). The latter result is significant because that means that we get to do both tasks for half the compute. In addition, they outperform all other single-model entries in the recent COCO Panoptic Segmentation Challenge. Note that later (2020), the transformer for Object Detection is pubished and ouperforms PanopticFPN."}}},"id":{"S":"98adaaec-0f39-4c32-9f19-1b3b7f801607"},"authors":{"S":"Kirillov, A., Girshick, R., He, K., & Doll\xe1r, P."},"title":{"S":"Panoptic feature pyramid networks"}},{"date":{"S":"2020"},"journal":{"S":"International conference on machine learning "},"labels":{"L":[{"S":"Contrastive learning"},{"S":"Unsupervised learning"},{"NULL":true}]},"sections":{"M":{"how":{"S":"In this contrastive learning setup, an image is transformed in two different ways. These two resulting data points are called a \u201cpositive pair\u201d. Each will be inputted to the same neural network (composed of a representation network and a projection head network) that will extract a feature vector for each of them. \\n\\nWe want the extracted representations of two transformed images to agree with each other.   The contrastive learning task will have a loss function that is going to maximize the \u201cagreement\u201d between these two feature vectors. \\n\\nWith these representations, we can then use these for linear classification. Or we can use the obtained network as an initial point for fine-tuning or transfer learning procedures.\\n\\nThe authors try out many different conditions, including different combinations of data augmentations, different architectures, and loss functions. \\n\\nThe end goal of experimenting with these different conditions is to determine which one leads to the best representations for a subsequent supervised learning task. They use a \u201clinear evaluation protocol\u201d which is going to train a classifier directly on these extracted representations, from the different settings. The better the final performance on the classification task, the better the representation is thought to be."},"why":{"S":"The contrastive learning idea (of making representations of an image agree with each other) is not new; it dates back to a Becker & Hintor paper in 1992. But previously proposed contrastive learning requires specialized architecture or a memory bank. \\nThis new framework removes the need for that. They spell out the necessary elements for having a successful simple contrastive learning procedure.  "},"what":{"S":"SimCLR algorithm is composed of three steps: (1) transformation, (2) representation, (3) projection. The latter two steps are done by a neural network. The projection neural network is typically one or two linear (or non-linear) layer.\\n"},"results":{"S":"Learning representations in the unsupervised setting benefit from:\\n- Composing augmentations, particularly the combination of random cropping and random color distortion. Unsupervised learning benefits more from this than supervised counterparts.\\n- Augmentations can replace the design of complex architectures as previously done in contrastive learning.  For example, random cropping of  an image can easily provide adjacent viewers and global/local views for the object in that image. \\n- Increasing depth and width. Unsupervised learning benefits more from this than supervised counterparts.\\n- Adding a projection head improves the representation learned. If it\u2019s a non-linear projection it\u2019s better. However note that the representation used for the downstream task should be that of the layer before the projection head, not the one after (See Section 4.2). \\n- The contrastive training loss that resulted in the best performance is the NT-Xent loss (Normalized, Temperature-scaled cross entropy loss). They find that the use of l2 normalization and temperature scaling is crucial for the resulting representation.\\n- Training longer and larger batch sizes leads to better representations as well. That is because the network ends up seeing more negative examples, which facilitates convergence.\\n- Compared with state-of-the-art (other methods for learning representations): they consistently outperform them for the linear evaluation mode and in the semi-supervised mode (in which the model is fine-tuned over few data points in ImageNet). "}}},"id":{"S":"8f019f1f-93b5-42a6-ba48-13f140142777"},"title":{"S":"A simple framework for contrastive learning of visual representations."},"authors":{"S":"Chen, T., Kornblith, S., Norouzi, M., & Hinton, G."}},{"date":{"S":"2017"},"journal":{"S":"In\xa0International conference on medical image computing and computer-assisted intervention\xa0(pp. 399-407)"},"labels":{"L":[{"S":"Active learning"},{"S":"segmentation"}]},"sections":{"M":{"why":{"S":"Annotating images in biology costs a lot of time and usually requires an expert annotator. This is not ideal for deep learning, which requires a lot of annotations to work well. \\n\\nThey propose a method based on active learning which intelligently selects what training data points is needed. Using this method, they can train deep neural networks on smaller training dataset than those usually required and still achieve state-of-the-art. As a result less annotations are required from the expert. "},"how":{"S":"They propose a new CNN architecture that uses residual modules and batch normalizations to speed up the training and other smaller architectural changes. To reduce chances of overfitting during active learning, they reduce the capacity of their network by adopting an encoder-decoder structure. \\n\\nTo suggest new images for annotation, they first find the subset of images for which the FCN network is most uncertain. They do so using a bootstrapping approach, where multiple (four in their experiment) FCNs perform a prediction on the same image, and uncertainty is estimated from the amount of disagreement between these FCNs. \\n\\nIn addition, they aim to select images that are most representative of the images in their dataset. To do so, they measure the cosine similarities (using the FCN features) between candidate images and the images in the dataset and choose the k images that have the highest similarity scores. The final images selected for annotations are those that intersect in the set of uncertain images computed earlier and the set of highly representative images computed form the cosine similarity. As a result, the images being queries are (1) images for which the FCN is highly uncertain and (2) images that are highly similar to the rest of the training data and therefore useful to annotate."},"what":{"S":"They seem to be the first to propose active learning for biomedical image segmentation. They answer the question on which data samples (image) should be selected for querying the user for an annotation. \\n\\nThey use a deep learning approach to solve the active learning problem: a Fully Convolutional Network extracts features from a given input image and from these, it decides which regions in the image should be annotated next."},"results":{"S":"They test their method by actually having access to ground-truth labels to the whole dataset, but only \u201crevealing\u201d/using the labels when their framework suggests it and queries specific data points. They experiment both with random querying and querying using their proposed framework. They find that their network can achieve the same/better performance as what is obtained when training on the full dataset, but using only 50% of the training data. Performance degrades when using random querying, as expected. \\n\\nIt is important to note that the waiting time between two annotations stage, i.e. the time for the framework to query the user with new images to annotate, is 10 minutes on a Tesla P100 GPU. That sounds like a long time to me, especially because most experts would be working on their local non-GPU workstations when annotating. "}}},"id":{"S":"1e0d0663-6f83-42a3-b6f6-7bff161a06f5"},"authors":{"S":"Yang, L., Zhang, Y., Chen, J., Zhang, S., & Chen, D. Z."},"title":{"S":"Suggestive annotation: A deep active learning framework for biomedical image segmentation"}},{"date":{"S":"2021"},"journal":{"S":"arXiv preprint arXiv:2101.05224"},"labels":{"L":[{"S":"Unsupervised learning"},{"S":"Semi-supervised learning"},{"S":"Fine-tuning"},{"S":"Contrastive learning"},{"NULL":true}]},"sections":{"M":{"how":{"S":"Their proposed pipeline is the following. First, they do unsupervised learning on ImageNet or on the medical data using the classical contrastive learning approach, SimCLR. This approach uses data augmentation to generate positive pairs. Each image is augmented twice using either random crop, color distortion, or Gaussian blur. The representation obtained from a ResNet-like architecture is fed to a non-linear projection head, and the resulting representation goes to a contrastive loss based on cosine similarity.\\n\\nNext, if possible, they continue the unsupervised learning phase with multi-instance contrastive learning (MICLe) for additional self-supervised training on the medical data. In this case, the positive pairs are naturally obtained by having taken a given pathology from different viewing angles, lighting conditions, or different body parts.\\n\\nThird, with these representations, fine-tune your ResNet-like architecture on the few labels that are available for this medical classification task. "},"why":{"S":"Classification for medical imaging tasks is no easy feat because getting labeled data in medical applications is a pain. We need to develop Deep-Learning based methods that do not require lots of labeled data to work well. They focus on the case in which we do pretraining to learn representations, and then fine-tune the models on a task using few labels."},"what":{"S":"\\nThe authors investigate in detail what types of pre-training can help for learning good representation which can then in term provide high accuracy for classification tasks on medical image data, when few labels are available.  \\n\\nThey leverage the power of pre-training and study its impact on subsequent medical imaging classification tasks. They compare supervised pre-training (for example on the large labeled ImageNet dataset) and unsupervised pre-training (using contrastive learning).\\n\\nOne of the take-aways from this paper is that supervised pre-training on natural image datasets is not an ideal pre-training approach for subsequent medical tasks, and unsupervised pre-training should be favored instead, to lead to better representations that lead to better generalization and are more label-efficient in subsequent fine-tuning. "},"results":{"S":"They find that best results are obtained when both the ImageNet and the task-specific medical image dataset are used, rather than one or the other individually.\\n\\nFurthermore, they find that using MICLe instead of SimCLR consistency improves the resulting model performance.\\n\\nInterestingly, they show that doing supervised classification pre-training on the labeled ImageNet does not improve the performance as well as unsupervised pre-training does. It may be that the representations learned are too task-specific and do not generalize well. \\n\\nFinally, self-supervised models are most robust to domain shifts after having fine-tuned on a specific domain. This is in contrast with using a supervised pre-training approach on ImageNet, which does not do well under domain shifts. In additional, they show that using self-supervised pretraining instead of supervised pre-training results in needing less labels during the finetuning stage. "}}},"id":{"S":"02e08442-bd03-42e2-988e-d29c347a5b09"},"title":{"S":"Big Self-Supervised Models Advance Medical Image Classification"},"authors":{"S":"Azizi, S., Mustafa, B., Ryan, F., Beaver, Z., Freyberg, J., Deaton, J., ... & Norouzi, M."}},{"date":{"S":"2021"},"journal":{"S":"Nature Methods"},"labels":{"L":[{"S":"Cell segmentation"},{"S":"Bioimage analysis"}]},"sections":{"M":{"why":{"S":"Accurate cell segmentation is a crucial part of research, including segmentation of the cell border \u2014 not just nucleus. \\n\\nThe DSB 2018 Challenge gathered a large dataset of diverse nuclei with the objective to allow people to train a model on this dataset, a model that generalizes well across the different types of nuclei. A model based on Mask RCNN ended up doing very well. \\n\\nThe work of these authors has the same objective: train a generalist model that can accurately segment a diverse set of cell images. "},"how":{"S":"The main contribution is in changing the predictive task of the DNN and using vector field representations of the images as the ground-truth labels. The neural network does not learn the segmentation task, but instead learns to predict a flow field by outputting (1) the horizontal gradient of the image, (2) the vertical gradient of the image, (3) the probability which indicates if a given pixel is part of a cell. Supposedly this should allow the network to extract a more robust representation of cells. \\n\\nGiven this predicted flow field, we can lead each pixel of a cell towards the center of that cell. For each pixel, they run a dynamical system starting at that pixel location and follow the spatial derivative specified by the horizontal and vertical maps. \\n\\nTo obtain the training data (the flow maps), a heat diffusion simulation is applied on each masks obtained by experts. \\n\\nThe neural network is based on the UNet architecture. They train the neural network on a diverse set of images. Note that it\u2019s a relatively small dataset (616 images) \u2014 contrast that with TissueNet.\\n\\nThey obtain 3D segmentation by running CellPose on XY, XZ, and YZ slice to then form a 3D vector flow field. Then they apply the dynamic system as usual, and obtain a 3D segmentation mask. \\n\\n"},"what":{"S":"Models in the past have attempted cellular segmentation, too, but these methods don\u2019t generalize well. \\n\\n\\u2028The authors hypothesize that the failures of prior work is due to not using the right representation for cells and networks can not capture complicated shapes. This work changes the training task from predicting a segmentation map to instead predict a vector field map. The segmentation maps are obtained from post-processing the output of the neural network. \\n\\nThey make their CellPose model available online, in a web app, but also as a downloadable app with a GUI. "},"results":{"S":"To compare the CellPose framework, they train the StarDist and Mask-RCNN models on their specialized and generalized data. In both the generalized and specialized settings, the CellPose consistently outperforms these two models. \\n\\nNote that their framework involves very time-consuming post-processing. To predict the mask from the flow field, for example, they do 200 iterations for each pixel in the image. Furthermore, their \u201ctest time enhancements\u201d (see page 10) adds more computational time to the framework. \\n"}}},"id":{"S":"85041c8f-5e20-4a08-befd-c482a1276373"},"authors":{"S":"Stringer, C., Wang, T., Michaelos, M., & Pachitariu, M."},"title":{"S":"Cellpose: a generalist algorithm for cellular segmentation"}},{"date":{"S":"2020"},"journal":{"S":"In European Conference on Computer Vision (pp. 229-246). Springer, Cham."},"labels":{"L":[{"S":"Inverse problems"},{"S":"Iterative optimization"}]},"sections":{"M":{"how":{"S":"In a traditional optimization scheme, the update rule is given by x(t+1) = x(t) + g(xt, yt, y), where g() is an update function analytically derived from the energy function defined in the optimization objective function. In this work, g() is implemented by a neural network. Thus the neural network g() can be seen as computing a residual between the previous solution x(t) and the new solution x(t+1). By having yt and y as its input, the neural network can compute how well the currently predicted solution x matches the known forward model, and output a residual based on that. \\n\\nIn some sense, the neural network learns how to conduct an iterative update over the current solution, based on the known forward process and the current observation and the \u201cground-truth\u201d observation.\\n\\nTo make this clear, the authors are not using the neural network as a direct mapping from y to x and are instead still using traditional analytical techniques. The main difference is that the update function g(), usually derived by hand, is implemented and learned by a deep neural network. \\n"},"why":{"S":"There exists a family of problems called inverse problems, which are difficult to solve: given an observation, the objective is to recover the original input to the forward problem.\\n\\nThere exists two main approaches for solving inverse problems: analytical-based approaches, in which human knowledge is uses to regularize or constrain the solution and formulate an optimization problem; and deep-learning based approach, where the objective is to learn a mapping from the observation to the source. \\n\\nAnalytical-based approaches can be difficult to design, usually require many iterations to converge, and are sensitive to initialization. Deep Learning-based approaches often produce results that are not realistic or incompatible with the real observations. "},"what":{"S":"When solving the inverse problem analytically, there generally is a feedback mechanism during the algorithm that checks that we satisfy f(x*) = y, where x* is the predicted unknown at time t of the optimization, f() is the feedback mechanism, and and y is the observation. \\n\\nThe objective of this work is to use Deep Neural networks as part of the optimization algorithm. "},"results":{"S":"They apply their methods to three applications: pose estimation, illumination estimation and inverse kinematics. Comparing their approach with a method that does not use DL to implement the feedback update, they do better. For pose estimation, their method seems to be more robust to extreme poses. "}}},"id":{"S":"18fd4031-2fc1-4213-90f7-a14c8590e767"},"title":{"S":"Deep Feedback Inverse Problem Solver"},"authors":{"S":"Ma, W. C., Wang, S., Gu, J., Manivasagam, S., Torralba, A., & Urtasun, R."}},{"date":{"S":"2020"},"journal":{"S":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"},"labels":{"L":[{"S":"3D reconstruction"},{"S":"Unsupervised learning"},{"S":"Auto-encoders"}]},"sections":{"M":{"how":{"S":"The input is the 2D image, and the output are the four parameters (depth, albedo, viewpoint and lighting). This corresponds to a 3D internal representation, from which a 3D image can be rendered.\\n\\nThe symmetry is learned via two mechanisms:\\n- Once we have the outputs (albedo, light, depth, viewpoint) of the individual neural networks, It is possible to render a symmetric reconstruction by flipping, across the vertical axis, the albedo and depth maps. Symmetry is then learned by enforcing that this reconstructed symmetric image, once projected back to 2D, is equal to the input image. \\n- Also, they augment the model to reason about potential lack of symmetry in objects, by having it predict a probability map of whether a given pixel has a symmetric counterpart in the image. This probability map is used when computing the loss term.\\n\\nThe learning objective is reconstructive: the model is trained to reconstruct the input image. It learns to decompose the input image into a (depth, albedo, viewpoint and lighting) parameters, and input these to a renderer that outputs a 3D image. The loss component is going to apply a projection to the resulting 3D image and enforce that it matches the input image. \\nThere are two loss terms in the overall loss: the reconstructive loss that enforces that the image from the predicted parameters is equal to the input image, and a loss that enforces that the image from the symmetrically predicted parameters is equal to the input image. \\n"},"why":{"S":"There is a vast literature on the topic of 3D reconstruction, which includes non-learning-based and learning-based methods. In the learning-based approaches, it\u2019s often the case that the supervision signal uses 3D ground truth, or videos, or 2D keypoint annotations, etc. \\n\\nThis work never uses 3D labels (or related labels) as part of its supervision. It is completely unsupervised. It uses a single-view input image as its input (whereas other works utilize multiple views of that image to reconstruct the 3D). \\n"},"what":{"S":"The learning problem is reconstructive. Given an input image, the network\u2019s task is going to reconstruct the input image. Note that this network is faced with an inverse problem: given the observed input image, the objective is to recover the underlying parameters that make its 3D structure. \\n\\nAn additional guidance principle in their work is that of symmetry. They assume that many object categories have, at least in principle, a symmetric structure. This assumption will be incorporated in the overall loss function. \\n"},"results":{"S":"\\nThey test their methods on human face datasets, cat face dataset, and synthetic cars datasets.\\nAs a result of training, the resulting autoencoder internally decomposes the image into albedo, depth, illumination and viewpoint, without direct supervision for any of these factors. \\n\\nTheir evaluation metrics use the ground truth depth maps that they have in their dataset. Given the depth maps predicted by their model, they can compute the \u201cscale-invariant depth error\u201d (SIDE) metric as an evaluation metric. In addition, they use the \u201cMean Angle Deviation\u201d, again computed from the depth maps.\\n\\nThey can also do a qualitative comparison with SOTA by just comparing the quality of the 3D reconstructions, see for example Figure 7.\\n\\nVia ablation studies, they show that symmetry and illumination are crucial cues to have the model converge to a good reconstruction. \\n\\nTheir model even outperforms a 3D reconstruction method that uses 2D keypoint supervision as additional input (whereas they only use the single viewpoint input image). "}}},"id":{"S":"b703c537-6941-4381-b4ae-bd1659d761f4"},"title":{"S":"Unsupervised learning of probably symmetric deformable 3d objects from images in the wild"},"authors":{"S":"Wu, S., Rupprecht, C., & Vedaldi, A."}},{"date":{"S":"2021"},"journal":{"S":"arXiv preprint arXiv:2103.09108"},"labels":{"L":[{"S":"ImageNet"},{"S":"Transfer Learning"},{"NULL":true}]},"sections":{"M":{"how":{"S":"The datasets that they use for their study are: (1) images of concrete buildings, (2) images of reef habitats, (3) images of ImageNet, (4) dermatoscopic images, (5) images of powerlines in Turkey, (6) insects, (5) natural scene dataset, (7) Cifar10/100 dataset.  This is a very diverse set of datasets. \\n\\nThe architectural space that they investigate are based on a single architecture, AnyXNet, and its variants. The stem and the head are the same across all architecutres, but the body will vary. The body is made of standard residual bottlneck blocks. The body is parameterized by d the number of blocks, w the block width, b the bottleneck ratio and g the group width (this corresponds to how many parallel convolutions the total width is grouped into). The AnyXNet architecture\u2019s design space has a total of 16 degrees of freedom, having four stages which each have four parameters (d, w, b, g).\\n\\nThey repeatedly sample the space to obtain a total of 500 architectures. All 500 models are trained on each dataset.  "},"why":{"S":" This work establishes the prevalence of the ImageNet dataset in Deep Learning papers. When studies propose new SoTA architectures, the obtained results are based on the ImageNet dataset. But is the ImageNet dataset really appropriate to establish new baselines? Do the observed results generalize across other non-ImageNet visual datasets? This work provides an answer to this question. "},"what":{"S":" They gather multiple datasets and study whether improvements seen when testing on the ImageNet models generalize to other datasets. "},"results":{"S":" They show scatterplots for each dataset, where a point corresponds to the test error of a sampled architecture trained on that dataset vs the test error of a sampled architecture on ImageNet. A linear correlation in the scatterplot would imply that optimizing architectures based on ImageNet performance results in improved performance on other datasets as well. The scatterplots show that for some datasets, such as insect or Cifar100, there is a positive correlation. However, for other dataset (such as powerline or the natural scenes dataset), there is a negative correlation! In such cases, it\u2019s not recommended to optimize things based on performance on ImageNet. \\n\\nWith additional studies, they show that a network depth and width found to be optimal on a certain dataset will not necessarily apply to another dataset \u2014 this is something important to keep in mind when doing transfer learning tasks.\\n\\nAnother interesting finding is that the generalizability of performance between datasets is related to the different number of classes in each dataset. Training on the ImageNet dataset with a subset of its classes (chosen to be similar to the target dataset) instead of its 1000 classes does lead to linear correlation. This implies that  one of the issues with the ImageNet dataset is its large number of classes compared to the number of classes included in the target dataset.  Thus if doing pre-training on ImageNet, it might be helpful to vary the number of classes and only use a subset of its classes.\\n"}}},"id":{"S":"bcc00206-46c3-4003-95f4-7b91c4e27e93"},"title":{"S":"Is it Enough to Optimize CNN Architectures on ImageNet?"},"authors":{"S":"Tuggener, L., Schmidhuber, J., & Stadelmann, T."}},{"date":{"S":"2020"},"journal":{"S":"arXiv preprint arXiv:2006.10029"},"labels":{"L":[{"S":"Unsupervised learning"},{"S":" semi-supervised learning"},{"S":" contrastive learning"},{"S":" distillation"}]},"sections":{"M":{"how":{"S":"The first step is to perform unsupervised learning with contrastive learning. In this step, general task-agnostic visual representation are learned. Their approach is the same as the one detailed in the SimCLR paper with the exception of: (1) they use a much deeper model, ResNet-152 instead of Resnet-50, (2) they use a deeper projection-head g(), (3) they make use of momentum contrastive learning (MoCO) detailed in He et al., 2020. \\n\\nThese general representations are then adapted with the supervised fine-tuning step, using the small number of labels available, and training for the desired task. Note that unlike SiMCLR, they do not throw away the projection head during the fine-tuning phase but instead fine-tune from the middle layer of the projection head. They emphasize that this change has had a major positive impact on the performance. \\n\\nThe third step uses distillation, which has the effect of (1) further improving the performance on the desired task and (2) obtaining a light, compact model. A small student network is trained to predict the same output of the fix large teacher network. This step uses unlabeled datasets because the teacher is the one producing as many labels as desired. "},"why":{"S":"This paper provides another unsupervised/semi-supervised learning framework for obtaining good performance on tasks in which few labels are available.  \\n\\nTheir unsupervised framework corresponds to an improved version of SiMCLR, named SimCLRv2."},"what":{"S":"Their new proposed framework consists of three phases: \\n(1) Unsupervised learning on large unlabeled datasets. Note that for this step they proposed an enhanced SimCLR framework, SimCLRv2. \\n(2) Supervised fine-tuning on the small number of labels that are available for the task at hand. \\n(3) Distillation to transition from the obtained deep and wide model to a small lightweight model. "},"results":{"S":"They evaluate their results on the ImageNet classification dataset. For unsupervised learning, they use the whole dataset, but for the supervised fine-tuning they experiment with only 1% or 10% of the dataset. \\n\\nThey show that using a much bigger ResNet model leads to the framework being more label-efficient (10-folds) for the semi-supervised learning case. Using a bigger and deeper projection head and fine-tuning from a middle layer of the projection head also helps getting better representations. Finally, the distillation step results in improved performance on ImageNet. \\n"}}},"id":{"S":"26d6b7a0-8e4e-4327-9276-59733ed4c687"},"title":{"S":"Big Self-supervised models are strong semi-supervised learners"},"authors":{"S":"Chen, T., Kornblith, S., Swersky, K., Norouzi, M., & Hinton, G."}},{"date":{"S":"2020"},"journal":{"S":"International Conference on Machine Learning (pp. 1691-1703)"},"labels":{"L":[{"S":"Unsupervised learning"},{"S":"Generative models"},{"S":"Transformers"}]},"sections":{"M":{"why":{"S":" The current SoTA for image-related problems with Deep Learning has focused on solving the problems in a supervised fashion, utilizing large datasets that are available out there. \\n\\nRecently, however, there has been interest in learning representations of images in an unsupervised methods. These learned representations can then be used for linear classification, or for fine-tuning on limited amount of labeled dataset."},"how":{"S":"They experiment with two different pre-training objective functions. The first one is the auto-regressive objective function, which  models p(x) as the product of all conditional probabilities p(x_i | x_{1\u2026i-1}). In essence, each pixel is predicted by looking at all the other pixels previously predicted. The second objective function is BERT\u2019s masked training objective, which is going to ask the model to predict masked pixels given unmasked pixels. \\n\\nThe input to their network is a flattened sequence corresponding to the pixels of the image. The decoder transformer produces a d-dimensional embedding for each input token (pixel), and on top of that has a stack of L blocks, each of them producing intermediate embeddings of size d. Because the image is flattened, the attention layer in the decoder is responsible for leaning to identify the 2D structure. Note the difference with DETR paper: they provide positional encodings to the input. \\n\\nBecause of the memory requirements of the transformer decoder, it would be impossible to feed in the whole sequence of length (224 x 224 x 3). To resolve this, the authors do two things: (1) they downsample the image to a smaller size, and (2) they switch from RGB to just a 9 bit color palette. The resulting context length has either (32 x 32), (48 x 48), or (64 x 64) elements. \\n\\n** Note: the Vision Transformer (Dosovitskiy et al., 2020) paper do apply a Transformer on a whole image size. Unlike this paper, the transformer model is trained for a discriminative visual recognition task instead of being trained as a generative model."},"what":{"S":" This paper is inspired by the success that NLP has had with unsupervised learning They borrow the GPT-2 architecture, more particularly its transformer decoder, and apply it on images instead of text modality. \\n\\nUnlike contrastive learning which use auxiliary tasks to learn representations, the representations are learned here using generative image modeling. They explicitly learn to model the distribution of pixels in an image."},"results":{"S":" They design GPT models of different sizes. They train on the ImageNet ILSVRC 2012 dataset. \\n\\nWhen working on the linear probing task, they experiment with using features of different layers in the model (not necessarily the last one). They find that choosing middle layers have the best features for good linear probing performance, and performance decreases as we choose deeper features. Note that when fine-tuning on the classification task, they attach a classification head on the layer with the best representations ( a middle layer), not the last one. \\n\\nWhen fine-tuning on the CIFAR datasets, they do really well. That\u2019s because those images have small resolution, and their model did well at learning the distribution of images of small spatial extent.\\n\\nThey don\u2019t do great on fine-tuning on the ImageNet dataset.  Clearly a limitation of this setup is that because of the dense self-attention on the input, they are constrained to using low-resolution inputs. It\u2019s difficult to learn high-quality representations in this case, and contrastive learning approaches (which are not constrained by this, since they use CNNs), will do better. "}}},"id":{"S":"e82d6aab-84fb-4db4-b2ea-86ee6f424344"},"authors":{"S":"Chen, M., Radford, A., Child, R., Wu, J., Jun, H., Luan, D., & Sutskever, I."},"title":{"S":"Generative pretraining from pixels"}},{"date":{"S":"2021"},"journal":{"S":"bioRxiv"},"labels":{"L":[{"S":"Cell segmentation"},{"S":"Bioimage analysis"},{"S":"Instance segmentation"},{"S":"Crowdsourcing"},{"NULL":true}]},"sections":{"M":{"how":{"S":"The TissueNet dataset was gathered using a supervised crowdsourcing, human-in-the-loop pipeline. This approach consists of multiple phases. In Phase 1, an expert annotates a small number of cells (~80), which are then used to train a preliminary model. In phase 2, given the output of the preliminary model, crowdsourced annotations (non-experts) make corrections. The corrected annotations are inspected by experts, and then added to the training dataset which is then used again to train the preliminary omdel. This continues with the model being re-trained with this new dataset, corrections are made again, verified by experts, etc. At the third phase, the model can generate predictions without any human supervision. This approach results in a huge dataset compared to what is already published: more than 1 million annotations.\\n\\nDeepCell Label is their platform for performing these annotations in the loop.\\n\\nThe Mesmer model is based on the PanopticNet architecture. Instead of the traditional two semantic heads for PanopticNet, their Mesmer has four semantic heads: two for nuclear segmentation, two for cell segmentation. They share the common ResNet backbone and FPN. \\n\\nThe input to Mesmer has two channels: a nuclear image (e.g., DAPI), and a membrane/cytoplasm image (e.g. CD45). The output of Mesmer is the boundary prediction and centroid predictions for every nucleus and cell. This is fed to the watershed algorithm for the final instance segmentation mask. \\n"},"why":{"S":"Accurate whole cell segmentation is crucial for understanding and quantifying spatial organization of tissues.  Inaccuracies at this stage can have bad consequences on the downstream analysis pipeline.  \\n\\nCell segmentation in tissue is a difficult talk for multiple reasons. These reasons include that cell morphology can vary widely from one tissue to another, and cell density is also often different (from densely packed to rare). \\n\\nDeep learning applied to bioimage analysis has become the popular approach. But for the task of whole-cell segmentation in tissue, very few annotated datasets exist for whole cell segmentation: most public datasets are for nuclei segmentation only (nucleis are easier to annotate). As a result, there does not exists any good DL model for whole cell segmentation. "},"what":{"S":"Their work introduces a new dataset, TissueNet, that contains whole-cell and nuclear annotations for tissue image. It contains more than 1 million of whole cell and nuclear annotations from nine different organs.\\n\\nUsing this dataset, they train a model named Mesmer. They integrate it as part of the DeepCell web app, making it easily accessible to biologists who wish to try it. \\n"},"results":{"S":"Compared with other deep learning models for tissue segmentation (e.g. FeatureNet, CellPose), they find that their model is faster and more accurate. Especially when compared with CellPose, which involves a lot of post-processing.\\n\\nThe model predictions are good across a range of tissue types. Errors were unbiased: errors were both on cells that were large, and small. They found that their generalist model (Mesmer) was competitive with specialist models, those that were trained on a specific tissue data type. \\n\\nHuman-level performance on cell segmentation. \\n\\nThey apply their Mesmer models on two research applications: (1) Quantifying cell morphology change during human pregnancy, (2) Segmentation for accurate downstream analysis of tissue imaging data. \\n"}}},"id":{"S":"b8d070f2-8e45-4ab2-8b4f-e520441f86cb"},"title":{"S":"Whole-cell segmentation of tissue images with human-level performance using large-scale data annotation and deep learning"},"authors":{"S":"Greenwald, N. F., Miller, G., Moen, E., Kong, A., Kagel, A., Fullaway, C. C., ... & Van Valen, D."}},{"date":{"S":"2018"},"journal":{"S":"Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"},"labels":{"L":[{"S":"Unsupervised learning"},{"S":"Supervised learning"},{"S":"Memory bank"}]},"sections":{"M":{"how":{"S":"The task becomes the following: each instance is its own class. In some sense we now have a \u201csupervised\u201d classification problem. If there are 1.2 million images in ImageNet, then we train the neural network on 1.2 million classes. The network learns a representation that is a vector with 128 elements only. \\n\\nThe probability that a given feature vector corresponds to class i (instance i) is given by the softmax equation, which is based on the inner product between the feature vector and the vector representation of instance i (see Equation 2). Note that the classic softmax formulation involves an inner product between the weights vector and the vector representation of the instance, making the learned model dependent on the classes assumed at the time of training. In the non-parametric case in Equation 2, the probability any new feature vector belonging to a specific instance can be computed. \\n\\nThe obvious problem is that this is computationally intractable (to say the least). They use two tricks to alleviate these computational concerns: \\n\u2014 The first trick is to approximate the full softmax with Noise Contrastive Estimation. NCE is not a new method \u2014 it has been used before in the literature to reduce computation, but they apply it to their problem. NCE reduces computational complexity from O(n) to O(1) per sample. A second they tricky \\n- The second trick is to use proximal regularization methods to stabilize learning process. Their regularization enforces that the representation of a specific feature at time t - 1 is not too different from the new computed one at the current inference pass at time t. \\n\\nAt each step of the training, a forward pass is computed and a new representation for instance i is obtained. The representation of that instance in the memory bank is updated. The loss and gradients are computed (matching each instance in the batch to its corresponding instance class), and the network parameters are updated to produce a better representation at the next step. The batch size used during training is 256 (but the class vector is 1.2 millions).\\n\\nThe resulting memory bank is not as large as one might think. Since they set the features of images to 128, the memory bank in the end is only 600 MB."},"why":{"S":"This work is another method for unsupervised learning of representations, which can then be used directly for a discriminative task, or used for fine-tuning supervised tasks. The latter application is particularly important as it is often that ground-truth data is hard to get in applications.\\n"},"what":{"S":"They argue that a useful embedding can be learned in an unsupervised manner by teaching a neural network to discriminate between instances. The network can then learn similarities among instances. Instead of learning \u201cwhat makes a class a class\u201d, they are teaching the network to tell us \u201cwhat makes an image an image?\u201d \\n\\nThese representations can then be used to construct a memory bank with known labels, and do subsequent classification with K nearest neighbors. Or these representations can be fine-tuned for another task, or used in a linear prediction problem. They test their representations on semi-supervised learning tasks on ImageNet, and fine-tuning tasks for object detection on PASCAL VOC. \\n"},"results":{"S":"They test their representation in two different settings: (1) Linear SVM on intermediate features of their network, from conv1 to conv5 and (2) KNN on the output features (size 128). In this second test case, they first compute its feature representation with the neural network, and then compare it against the embeddings of all the images in the memory bank, using the cosine similarity function. The top k nearest neighbors are used to make the decision via weighted voting. \\n\\nThey show that their learned features does not work well in the linear classification setting, when a SVM is trained to do a classification directly on these features. This is expected, as the learned features do not correspond to semantic features. Hence the SVM is not working on a linearly separable space where classes have been separated from one another. It makes sense that KNN would be much more appropriate for this application (and convenient as there are only 128 features). Under the KNN test setting, their method outperforms the other unsupervised methods. \\n\\nSemi-supervisd setting is when we first learn from a large unlabelled data (as they did) and they use a small amount of labeled data to fine-tune the model. They train only on a small amount of data of ImageNet, from 1% to 20%. They compare their methods against other pre-training methods called \u201cSplit-brain\u201d and \u201cColorization\u201d. They consitently outperform both of them. \\n\\nFinally, they apply their learned features in the Object Detection setting, initializing the backbone networks with their learned weights and fine-tuning these for the object detection task. They compete well with the other unsupervised methods who have also finetuned on Object Detection tasks. If choosing ResNet-50 as their backbone, they outperform the other unsupervised methods. "}}},"id":{"S":"cb2f7629-808e-4817-9469-e29f1f5a49c5"},"title":{"S":"Unsupervised feature learning via non-parametric instance discrimination"},"authors":{"S":"Wu, Z., Xiong, Y., Yu, S. X., & Lin, D."}},{"date":{"S":"2020"},"journal":{"S":"arXiv preprint arXiv:2010.11929"},"labels":{"L":[{"S":"Supervised pre-training"},{"S":"image classification"},{"S":"transformers"}]},"sections":{"M":{"why":{"S":"This work proposes a new architecture for solving the image classification task, instead of using Convolutional Neural Networks. Their model is abbreviated ViT (Vision Transfomers)"},"how":{"S":"They want to apply as few modifications as possible to the original Transformers architecture. To do so, they take patches of the image of size P x P x C and apply linear embeddings of each patch. Each of these linear embeddings corresponds to a token in the sequence fed to the Transformer architecture. The embedding is dimension D and stays that dimension throughout each layer of the Transformer. Note that they also experiment with using features extracted from a CNN instead of operating on the raw image directly. \\n\\nNote that the first token inputted to the transformer is a classification token (not a classification label, just a token). See Figure 1. This is necessary so that the corresponding first token at the output of the transformer encoder  will be used as the aggregate sequence representation for the classification task. That is why the classification head takes in that output token only. \\n\\nDuring pre-training, position embeddings are added to each patch embedding to retain position information. During the fine-tuning task, they perform 2D interpolation on the pre-trained position embeddings, according to their location in the original image. \\n\\nThey do not train the Transformer from scratch and instead pre-train it in a supervised way for a classification task on very large datasets. Then they fine-tune on the desired classification task by throwing away the classification head used during training and replacing it with a layer of the size of number of classes for this new task. "},"what":{"S":"Previously all the other papers that have attempted to use transformers on image-related tasks so far have done so by inserting transformers into a CNN framework on some way or another. For example, some have stacked up transformers onto a CNN backbone (think DETR), or others have put attention layer within CNN layers (think the bottleneck transformers).\\n\\nWhat\u2019s cool about this paper is that they scratch out CNNs completely. It\u2019s the first transformer to be applied to whole size images, replacing the role that CNNs usually takes, and that can be trained on classification task. Note that the \u201cGenerative pretraining from pixels\u201d paper has already successfully applied transformers on images, but is different in that: (1) they applied transformers on downscaled images instead of full size images and (2) their objective was to learn representations in the context of unsupervised learning. This ViT paper is the first to apply Transformers with global self-attention to full-sized images, with no help from CNNs whatsoever (although they do experiment with using features from CNNs in their hybrid model)."},"results":{"S":"They compare their ViT model with a modified ResNet architecture that is more appropriate for transfer training setups like this one, inspired by the ResNet Big Transfer (BiT) paper. \\n\\nTheir ViT-Huge performs better than the ViT-Large. Note that pre-training ViT on the very large JFT-300M dataset works much better than the smaller ImageNet-21k dataset. Under those settings, they outperform BiT-L and Noisy Student. \\n\\nAnother interesting observation is that pre-training ViT in a self-supervised fashion, using a masked patch prediction loss (similar to BERT), does help improving accuracy compare with training from scratch, but still performs worse than if ViT has been pre-trained in a supervised manner. \\n\\nFuture work includes generalizing ViT to object detection / segmentation and to explore other sel-supervised pretraining approaches such as the use of Contrastive Learning with ViTs."}}},"id":{"S":"f6724be3-62c5-4a88-9c2b-966c7179aea0"},"authors":{"S":"Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., ... & Houlsby, N."},"title":{"S":"An image is worth 16x16 words: Transformers for image recognition at scale"}},{"date":{"S":"2017"},"journal":{"S":"arXiv preprint arXiv:1703.05175"},"labels":{"L":[{"S":"Few-shot learning"},{"S":"Zero-shot learning"},{"S":"Episodic training"}]},"sections":{"M":{"how":{"S":"They assume that each class has a single prototype representation. The class\u2019s prototype is just the mean of its support set in an embedding space. The embedding space is learned by the prototypical neural networks. At test time, for a given query data point, we compute its representation in the embedding space using the trained prototypical network, and find the nearest class prototype.\\n\\nIn zero shot learning, a class comes with meta-data instead of labeled examples. For zero shot learning, the authors propose to learn a separate embedding of the meta-data itself. Given a new test point, we find the nearest class prototype, as in the few-shot learning case. \\n\\nThe training process mimics the few-shot problem, using episodes. During training, a subset of classes are sampled, and some images of that class are used as the support set to estimate a prototype (the mean of the embeddings of the support points), and other images of that class are used as the queries (classifying new, unseen example). With each gradient update, we update the embedding model f() so that it produces a better embedding space, hence a better prototype for whatever class we have at the next episode.\\n\\nThe key thing here is that during training, the support set is small and only a few classes are samples. Therefore the network is trained too solve the few-shot problem directly. "},"why":{"S":" The few-shot classification problem is when a classifier must be adapted to be able to predict new classes not seen in the training set, given only a few examples of each new class.\\n\\nI emphasize here that typically in few-shot learning, no fine-tuning is preferable (no retraining of the classifier parameters whatsoever). The training objective is to learn a model that can output a robust representation even from just a few examples (the support set) of a new class, and use this representation for new query points. The training objective is not to provide a mapping from an input image to a class."},"what":{"S":" Many current approaches for few-shot learning employ a particular approach during training: mini-batches called \u201cepisodes\u201d are used during training, where each episode is designed to mimic the few-shot task by subsampling classes as well as data points. This is called \u201cepisodic training\u201d.  The popular \u201cmatching networks\u201d (Vinyals et al.) uses episodic training. This work also uses episodic training.\\n\\nAnother SoTA method for few-shot classification is the meta-learner LSTM. If I remember correctly, a separate optimization problem is solved for each new unseen class.\\n\\nThis is interesting because it means that the task learned by the neural network during training is the few-shot classification task itself. \\n\\nTheir work introduces prototypical networks, a new method for one shot and few shot learning that is simpler and more efficient than recent meta-learning algorithms, and achieves SoTA performance.  The key contribution is that a simple \u201cprototype\u201d for each is class is assumed. The prototypical network is trained to produce the embedding space in which this prototype is compute."},"results":{"S":" They find that the choice of distance is critical:  the use of Euclidean distance greatly outperforms the generally used cosine similarity distance. \\n\\nThey seem to outperform matching networks and Meta-learner LSTM on the miniImageNet problem and the Omniglot dataset.\\n\\nThey obtain SoTA performance on the CUB-200 dataset for zero-shot learning."}}},"id":{"S":"f445a470-9bd9-4e77-a612-6d3d9da34b69"},"title":{"S":"Prototypical networks for few-shot learning"},"authors":{"S":"Snell, J., Swersky, K., & Zemel, R. S."}},{"date":{"S":"2020"},"journal":{"S":"arXiv preprint arXiv:2004.11362"},"labels":{"L":[{"S":"Supervised learning"},{"S":" contrastive learning"}]},"sections":{"M":{"how":{"S":"The objective of their supervised contrastive loss is to pull together images of the same class, and pull apart images that are not in the same class. Unlike traditional contrastive learning where there is typically one positive pair (the two data augmented examples), here there are many positive samples (any instance that belongs to that class) with the many negative samples.\\n\\nThe main difference between their supervised contrastive loss and the traditional self-supervised contrastive loss is that, somewhere in that loss, we should account for the fact that there is more than just one positive example given an anchor point. This means that a summation sign (summation over all positive examples in the batch) should be added into the loss. The authors experiment with having that summation both inside and outside of the log term, and find that having it outside leads to better results. This experimental validation is further confirmed with their analytic assessment on gradients of that loss . \\n"},"why":{"S":"These authors provide a new pre-training method for computing representations which can then be used for subsequent classification task. Their pre-training method addresses the supervised learning case and not the unsupervised learning, as it leverages labels from the ImageNet dataset. \\n\\nThe idea behind this method is that the resulting learned embedding space obtained from the pre-training phase separates the instances of different classes well already,  so that these objects be easily classified with a linear classification in subsequent classification tasks. In principe, this should lead to better classification results. "},"what":{"S":"They propose a new objective function for their pre-training method to obtain representations. This loss is a contrastive loss that is changed for the case in which we do have access to class labels in the dataset. \\n\\nNote that some of the introductory text in this paper is a bit misleading: they seem to imply at first that they propose a loss function to replace the cross-entropy loss function used in classification. That is not the case. They do propose a new loss function for learning representations with known labels and contrastive learning, but these representations are then fed to a linear classifier which does use cross-entropy loss. The classification task still uses the cross-entropy loss only. Their new loss is only for the embedding space obtained prior to the classification task.\\n"},"results":{"S":"The normalized activations of the final pooling layer in the encoder are used as the representation vector for their subsequent experiments. (The projection head used during contrastive learning has been thrown away at that point. Note that a SimCLRv2 paper has found that in some cases it might be best to keep up to the middle layer of the projection head). This representation vector is trained with a linear classification layer using the cross-entropy loss.\\n\\nTheir table 2 shows that their supervised contrastive loss leads to better final classification accuracy than self-supervised contrastive loss or training on the categorical cross-entropy directly. \\n\\nThey investigate different depths of ResNet architecture as the encoder model with different augmentation frameworks. Their ResNet-200 encoder model achieves almost a 1% improvement over the state-of-the-art when using the Stacked RandAugment augmentation procedure.\\n\\nFurthermore the authors show that this training framework comes with increased robustness to different image corruptions when measured with mean Corruption Error (mCE) and relative CE\\n\\nFinally, interestingly, they find that their method does not work better than cross-entropy or self-supervised contrastive methods when fine-tuned on 12 other natural image datasets. This is consistent with other things that you have read \u2014 and reminds me of the Tuggener et al. (2021) paper on whether it is enough to optimize CNN architectures on ImageNet. \\n"}}},"id":{"S":"5b7f4982-1f0c-4f1e-81fa-1c1d80038f0c"},"title":{"S":"Supervised contrastive learning"},"authors":{"S":"Khosla, P., Teterwak, P., Wang, C., Sarna, A., Tian, Y., Isola, P., ... & Krishnan, D."}},{"date":{"S":"2020"},"journal":{"S":"European Conference on Computer Vision"},"labels":{"L":[{"S":"Object detection"},{"S":"Transformers"},{"NULL":true}]},"sections":{"M":{"how":{"S":"It\u2019s a set prediction problem, so the target is going to be a set of size N, containing tuples of the (class, bounding box) of the instances in that image, and everything else in that set will be the \u201cno object\u201d label. This \u201cno object\u201d label can be seen as a way to pad the set. The architecture will be such that it outputs a set of that same size N. Again, the architecture is able to predict the \u201cno object\u201d label. The important thing to understand is that the target prediction is a set of N instances (most of them will correspond to \u201cno object\u201d), and the proposed prediction is a set of N instances as well.\\n\\nThe biggest challenge when formulating things as a set prediction problem is to match the prediction of the network to an instance in the target set. The way this is achieved here is with a \u201cbipartite matching loss\u201d. It uses the outputted classes and bounding box locations to try to match each prediction instance to another instance in the target set. Note that this results in a one-to-one matching, so we can be sure that multiple predicted bounding boxes will not be mapped to the same instance in the target set. That\u2019s why NMS is not needed in this workflow. \\n\\nOnce the predictions are matched to the target instances, we use a Hungarian loss to actually train the neural network. This loss is very intuitive and similar to simple classification loss used in object detection. There is an extra novelty that they use a generalized IOU loss on the bounding box loss component, which helps making the loss a bit more scale invariant. \\n\\nThe user of a transformer architecture is the key distinguishing feature of their paper. Figure 2 in their paper shows the following:\\n- An initial CNN, named \u201cbackbone\u201d CNN is going to extract features from the initial image. There is going to be more channels and a smaller spatial extent. Then this feature map of size C x h x w is going to be flattened into a further compressed vector of size d x (hw). That\u2019s the sequence that\u2019s going to be fed at the input of the encoder. In some sense, each \u201ctime step\u201d corresponds to a pixel in the image (with a feature vector representation of size d).\\n- The encoder part of the transformer takes in this image sequence, in addition to positional encoding information. This is necessary because all information about position has been lost when we flattened the image and feed that to the encoder. The encoder then is going to output feature representations of those images. At this point, the encoder should have learned to focus on specific parts of the image to extract different instances.\\n- The decoder part of the transformer is going to look at the extracted features from the encoder and output a set of bounding boxes. The decoder architecture also takes in \u201cobject queries\u201d. It is necessary to input object queries to the decoder: to output N things, we need to input N things to it. These are learned during training. When combining information from feature maps from the object query, and feature maps from the encoder, then we can obtain bounding boxes."},"why":{"S":"In most traditional CNN-based methods for object detection, the workflow isn\u2019t end-to-end and involves some amount of \u201cpreprocessing\u201d or \u201cpostprocessing.\u201d \\n\\nFor example there are a lot of works (including R-CNN-based methods) that are going to pose \\\\nthe problem as first predicting a set of candidate boxes, and then classifying these boxes. These types of preprocessing are not ideal, because it makes the performance of the overall model very dependent on the engineering decisions made. \\n\\nPostprocessing\u201d often includes the use of NMS methods to remove duplicate bounding box predictions. \\\\n\\\\nIn this work, the object detection workflow is much simpler. That\u2019s why their paper is called \u201cend-to-end\u201d object detection. That\u2019s a big thing. "},"what":{"S":"They choose to formulate the object detection problem as a set prediction problem. \\n\\nNote that they are not the first to approach the object detection problem as a set prediction problem. There exist methods where RNNs do that, but these methods are auto-regressive. This work here uses parallel decoding. (They make use of the recent work of parallel decoding with transformers)\\n\\nFurthermore, and that\u2019s where there is the most novelty: they use an encoder-decoder transformer architecture with parallel decoding to model the inference. "},"results":{"S":"The most interesting part of this paper is their section on visualizing the attention of the encoder and decoder networks.\\n\\nThey show that the encoder pays attention to the different instances in the image. Note that it does really well as separating close / overlapping instances. \\n\\nThe decoder clearly pays attention to the extremities of the object. It seems that it learns from the feature representation given by the encoder, and understands that extremities is what matters, and predicts a bounding box from that. \\n\\nAs a result of training, each \u201cslot\u201d (box) in the decoder becomes responsible for looking at a specific area of the image. This is shown in Figure 7. It seems like each slot is responsible for predicting a bounding box at a specific region of the image. \\n\\nIn terms of performance w.r.t RCNN and its different flavors, they show that they perform similarly (better for predicting large boxes, a bit worse for predicting small boxes). \\n\\nFinally, they show that they can easily apply their DETR model to the task of panoptic segmentation. All that is needed is a \u201cmask head\u201d (a CNN) to be appended to the features outputted by the decoder. The resulting network can be trained for panoptic segmentation (jointly, or not). In most cases, they do better than the previous SoTA PanopticFPN (which is based on a mask RCNN with an instance and segmentation branch) re-trained on their dataset. "}}},"id":{"S":"ae23f103-0cf4-49b4-88d5-cccfabac5ca8"},"title":{"S":"End-to-end object detection with transformers"},"authors":{"S":"Carion, N., Massa, F., Synnaeve, G., Usunier, N., Kirillov, A., & Zagoruyko, S."}},{"date":{"S":"2017"},"journal":{"S":"In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (pp. 5221-5229)"},"labels":{"L":[{"S":"Image segmentation"},{"S":"Instance segmentation"}]},"sections":{"M":{"how":{"S":"They aid the network by defining an intermediate task: have the network predict a \u201cdistance transform\u201d, which tells us for each pixel how far it is from its instance\u2019s boundary. This is performed with a group of layers named \u201cDirectionNet\u201d This is then passed to the next layers of the network, \u201cWatershed Transform Net\u201d, that predict the energy of the watershed transform. \\n\\nDirectionNet is pre-trained to estimate the direction of descent of the energy at each pixel. This results in a 2-channel unit vector map, where each pixel has a unit vector pointing to a specific direction. \\n\\nThis direction map is then fed to Watershed Transform Net, which outputs a discretized watershed transform map with 16 possible energy values. The higher the energy value at a given pixel, the more the pixel is located at the interior of the instance. Low energy values are for pixels near the boundary of the instance. With a single level cut on the energy map, they can obtain the segmented instances. \\n\\nThis idea of using an energy map reminds of StarDist\u2019s approach in predicting for each pixel, a distance to its instance boundary. \\n\\nThe input to the network is the image AND its semantic segmentation map. Note that this is an extra requirement that can be seen as a limitation. \\n\\nEach sub-network is pre-trained independently, and then the whole system is fine-tuned. "},"why":{"S":"This work proposes an end-to-end method for instance segmentation that combines classical image segmentation with learning-based image classification. \\n\\nThe current (in 2017) methods for Deep Learning segmentation usually involve a complex pipelines, for example involving Conditional Random Fields, or template matchings, or large RNNs. \\n\\nWatershed-based segmentation has the problem that it tends to over-segment the image. \\nThis method should supposedly combine the best of both worlds. \\n"},"what":{"S":"Traditionally, the watershed transform is applied on the pre-computed gradients of the image. However, it is often difficult to find sharp gradients that represent well the boundaries between different instances. Instead, they propose to use a CNN that directly predicts the energy of the watershed transform, given an image.\\n\\nThe predicted transform is constrained to have each basin correspond to a single instance \u2014 this is useful to prevent over-segmentation frequently observed in watershed (See Figure 2 for a great illustration). \\n"},"results":{"S":"They test their method on the Cityscapes Instance Segmentation benchmark. This is a difficult dataset because objects are often included, and there usually are dozens of instances in a single image. \\n\\nThey significantly outperform the other methods that they compare to. "}}},"id":{"S":"fa3daf7c-4290-4602-9d69-f89af9865659"},"title":{"S":"Deep watershed transform for instance segmentation"},"authors":{"S":"Bai, M., & Urtasun, R."}},{"date":{"S":" 2020"},"journal":{"S":"In European Conference on Computer Vision (pp. 268-285) "},"labels":{"L":[{"S":" unsupervised learning"},{"S":" clustering"},{"NULL":true}]},"sections":{"M":{"how":{"S":" Their method is a two-step approach. First, they use a pretext task to learn feature representations of images in an unsupervised manner. Second, they use these feature representations as a prior for their clustering procedure. \\n\\nThe pretext task can be one of the pretext tasks already available in today\u2019s literature, for example the task used for instance discrimination or for contrastive learning. The hypothesis here is that the similar features will be assigned to semantically similar images. \\n\\nThe clustering procedure starts with the mining of nearest neighbors in the space computed by the pretext task.  Each sample point and its found nearest neighbors is considered a \u201cground-truth\u201d cluster. With this data, they learn a clustering function that will be able to assign each new sample to a cluster c.  \\n\\nThe weights learned from training with the pre-text tasks are used as initialization for their clustering training. \\n\\nIn addition, they employ a \u201cfine-tuning\u201d procedure using self-labeling. In cases where the learned clustering mapping assigns a high probability to a given cluster c given a new sample point X, it is assumed that this prediction (X, c) is correct, and is used as additional \u2018ground-truth\u2019 labels in the clustering loss. This additional self-supervised fine-tuning corrects for mistakes done in the first part of the clustering training.\\n\\nTheir architecture used in all experiments is standard ResNet backbone. The number of clusters here corresponds to the number of known classes."},"why":{"S":" The objective of this work is to classify images without ever receiving labels. This is achieving by grouping images into semantically meaningful clusters"},"what":{"S":" This is different from the unsupervised + finetuning works that you have been reading, where representations learned in an unsupervised way are then fine-tuned with existing labels. "},"results":{"S":"  The obtained clustering accuracy is superior to other clustering state-of-the-art. This is an interesting method to keep in mind if there are no labels available at all for classification purposes."}}},"id":{"S":"1472d9df-b04b-4a2c-9326-a02889ed4793"},"title":{"S":" SCAN: Learning to classify images without labels"},"authors":{"S":" Van Gansbeke, W., Vandenhende, S., Georgoulis, S., Proesmans, M., & Van Gool, L."}},{"date":{"S":"2020"},"journal":{"S":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"},"labels":{"L":[{"S":"Unsupervised learning"},{"S":"Contrastive learning"},{"NULL":true}]},"sections":{"M":{"how":{"S":"Each forward pass provides a new set of keys for the current mini-batch. In addition, we still keep the previous encoded K keys in the queue as additional negative examples when computing loss and gradients. The gradient of the query network f_q utilizes all of the negative samples that are in the queue. To update the keys network f_k, though, we do not use backpropagation as things would become intractable \u2014 think of all the keys that are in the queue, each of them coming from that f_k, it is too many gradients. But we still want to update the keys f_k network, otherwise it becomes quickly out-of-date (as in the case of memory bank contrastive learning) and we might as well be comparing apples and oranges during training. To do so, they use a simple momentum update, which uses the weights of the queries network that are regularly updated by backprop. (See Equation 2). \\n\\nAt the end of a forward pass, the current representations of that mini-batch are enqueued in the dictionary, the oldest is dequeued.\\n\\nTheir method can be seen as an enhanced procedure for contrastive methods that use memory bank. Like the memory-bank-based methods, they keep a representation of all samples in the data (to some extent). By using the momentum update rule, and regularly enqueuing and dequeuing keys, they update the encoder network that results in the representations of those samples stored in the queue, on the long term. \\n\\n"},"why":{"S":"Using contrastive learning has been shown to be a successful way to learn representations in an unsupervised manner, which can then lead to better performance when using these representations for supervised tasks. Their work is meant to provide a better training mechanism that leads to better representations. These representations can be used as a linear classification protocol, or for transferring tasks."},"what":{"S":"The contribution is in the training mechanism in the unsupervised learning task. In the traditional contrastive learning, the number of negative examples provided during training are limited by the small mini-batch size. Thus the learning is limited by small batch sizes. In this work, they keep the previously encoded keys in a queue of size K keys, much larger than a mini-batch of size N. By using such a large queue, we get a large negative set of samples, which results in better learning.\\n"},"results":{"S":"Under the linear classification protocol, they even do better than contrastive methods that use memory banks, which keep a representation of ALL samples in the dataset. \\n\\nThey show that they have a good tradeoff between number of parameters and accuracy. Even their encoders with a relatively small number of parameters lead to good results (see Table 1).\\n\\nThey use their methods to do feature transferring for image classification, and object detection tasks. The trained encoder is the backbone for that object detection task. It is fine-tuned on the ground-truth labels that we have. Note that in Section 4.2, they write about how the distribution of features resulting from contrastive loss learning are not the same as features typically obtained from supervised learning \u2014 thus the typical hyperparameters used to train the same backbones on supervised tasks would be different. They decide to keep the same hyperparameters, but instead normalize the unsupervised features learned. \\n"}}},"id":{"S":"e0b488af-beaa-47ad-9d2d-63c56443509e"},"title":{"S":"Momentum Contrast for Unsupervised Visual Representation Learning"},"authors":{"S":"He, K., Fan, H., Wu, Y., Xie, S., & Girshick, R."}},{"date":{"S":"2018"},"journal":{"S":"arXiv preprint arXiv:1810.04805 "},"labels":{"L":[{"S":" Unsupervised training"},{"S":" NLP"},{"S":"Transformers"}]},"sections":{"M":{"how":{"S":" See Figure 2 for a representation of how the input looks. Each input token is the sum of a token embedding (e.g. WordPiece vocab), a segment embedding (necessary for distinguishing sentence A  from sentence B) and a position embedding. Multiple sentences are send in as a single input token, hence the need for a segment embedding. \\n\\nA first proposed training task is a masked language model (MLM), which randomly masks some of the tokens from the input, with the objective to predict the words of the masked tokens.\\n\\nThe second pre-trained task is that of next sentence prediction. This is a binary classification problem where we ask whether a given sentence is next to the current sentence, or not. This is done to obtain a model that understands sentence relationships. \\n\\nBERT\u2019s architecture is a multi-layer bidirectional Transformer encoder.  When finetuning, the appropriate inputs and outputs are given and all of BERT\u2019s parameters are fine-tuned end-to-end. \\n\\nNote that BERT\u2019s hidden state feature vector could be used as an input to a classification layer, just like in other unsupervised papers you have read. The authors test that out in Section 5.3. "},"why":{"S":" This work proposes Bidirectional Encoder Representations from Transformers (BERT), a model that provides a representation for text. This representation is obtained in an unsupervised manner and can be used as initial features for fine-tuning a variety of NLP tasks, such as question answering on language inference. "},"what":{"S":" The difference is in the proposed task that is used for this unsupervised learning, which allows for a a bidirectional flow instead of usually used left-to-right. This also means that bidirectional architectures can be used. \\n\\nThis is in contrast with GPT-1, for example, whose architecture relies on a left-to-right architecture where each token can only attend to previous tokens in the self-attention layers by using masking."},"results":{"S":" Note that the BERT baseline and the OpenAI GPTs have similar capacity. BERT-baseline seems to outperform GPT-1 on many different fine-tuned tasks when looking at GLUE metrics. The BERT-large performs even better, as expected. "}}},"id":{"S":"6b8732dc-0a70-4e1a-827a-2cd4f6523e92"},"title":{"S":" Pre-training of deep bidirectional transformers for language understanding"},"authors":{"S":"Devlin, J., Chang, M. W., Lee, K., & Toutanova, K. "}},{"date":{"S":"2015"},"journal":{"S":"In\xa0Proceedings of the IEEE conference on computer vision and pattern recognition\xa0(pp. 3431-3440)"},"labels":{"L":[{"S":"semantic segmentation"}]},"sections":{"M":{"why":{"S":"This paper, written in 2015, proposes a new end-to-end approach for the task of semantic segmentation. They achieve a new state-of-the-art on the PASCAL VOC dataset. "},"how":{"S":"Their architecture utilizes \u201cskip\u201d connection to combine coarser features with fine-grained ones. In this way, semantics can be combines in a local-to-global pyramid framework.  They later show that fusing information from layers with different strides results in improved segmentation detail. \\n\\nBecause the first convolutional layers downsample the feature maps, they need to find a way to upscale these back to the size of the input image to get the dense segmentation. They use backwards strided convolutions (also called deconvolution filters) to do so.\\n\\nThey compare training on patches of the images vs training on the whole images and found that patch sampling does not yield faster convergence or better results. They propose to train on the whole images instead. \\n"},"what":{"S":"This is the first paper to propose to use and end-to-end fully convolutional neural network, the FCN, for the task of semantic segmentation. Indeed, prior works for semantic segmentation include post-processing or pre-processing steps as well, such as region proposals or with the use of superpixels or by using patchwise training \u2014 they were not end-to-end. \\n\\nFCNs were previously exclusively used for the task of image classification. The authors show that we can apply these to the segmentation task and having full sized images as input to the network.\\n\\nFinally, they are the first to use transfer learning by starting with pre-trained weights prior to training their model on the segmentation task, instead of training from scratch. \\n"},"results":{"S":"In their training experiments, they first take existing pre-trained classifiers  (AlexNet, VGG16, GoogLeNet) and attach the network upsampling head to them to be able to obtain a dense prediction on the input image. Next, they add the skip connections to the architecture. They then fine-tune on the PASCAL VOC dataset. \\n\\nComparing with the at-the-time SoTA for segmentation, they improve a 20% relative improvement on the PASCAL VOC dataset. \\n\\nNote for myself: How is UNet different? UNet has a symmetric encoder-decoder structure, and the skip connections go from each encoder block to the corresponding block in the decoder.  This is different from FCN, where the upsampling step is a single step towards the end of the network.\\n\\nTODO: How are FPN different? [find out]"}}},"id":{"S":"6cc0b30e-209d-4c61-aea4-de5d4f528834"},"title":{"S":"Fully convolutional networks for semantic segmentation"},"authors":{"S":"Long, J., Shelhamer, E., & Darrell, T."}},{"date":{"S":" 2016"},"journal":{"S":" arXiv preprint arXiv:1605.09782"},"labels":{"L":[{"S":" Unsupervised learning"},{"S":" GAN"},{"NULL":true}]},"sections":{"M":{"how":{"S":" A traditional GAN consists of a generator network that takes in the vector z and produces an image G(z). In addition to this generator, the authors add another network, an encoder network, that will take in an image x and produce E(x), its features representation. \\n\\nIn this set-up, a discriminator will be trained for two tasks: (1) compare the produced image G(z) with the ground-truth images in the dataset, and (2) compare an encoded latent space vector E(x) and compare it with the original vector z. Similarly the generator and encoders\u2019s task is to fool the discriminator. Training is done by computing gradient steps for D, G, and E for each mini-batch and updating all modules simultaneously. \\n\\nIn all experiments, the architecture E is based on the AlexNet network. \\n\\nAn alternative task for the encoder E that is explored in the paper is have a loss function that requires the resulting vector E(G(z)) to be close to the latent z. In the results section, they show that the features in an E() trained in that way also contain helpful representations but are not as efficient as what is obtained with training on E(x). \\n"},"why":{"S":" This work proposes a method for learning representation of images, in an unsupervised manner. These features can then be used for subsequent supervised classification tasks. "},"what":{"S":"The proposed method for learning representations is new: it makes use of Generative Adversarial Networks and their ability to sample natural images from a small latent space. \\n\\nThe general hypothesis is the following: if GANs can produce an image from a small latent space, can we also train a GAN to project an image to this small latent space? Can this learned encoding be used as a feature space for subsequent supervised tasks?"},"results":{"S":" When fine-tuning for imageNet classification, they freeze the first N layers (they experiment with 1 to 5) of the BiGAN encoder and randomly initialize all the other weights to be retrained on ImageNet labels. They show that BiGAN is competitive with the other unsupervised learning methods that use the same AlexNet architecture.\\n\\nWhen fine-tuning for VOC detection, and segmentation, the AlexNet model is used as initialization for a Fast-RCNN and a FCN architecture. All of the layers are retrained. They show that the features learned are competitive with other existing methods for unsupervised representation.  "}}},"id":{"S":"170fd091-82cc-45a8-bc5e-a76a6e26b055"},"title":{"S":" Adversarial feature learning"},"authors":{"S":" Donahue, J., Kr\xe4henb\xfchl, P., & Darrell, T."}},{"date":{"S":"2019"},"journal":{"S":"33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada."},"labels":{"L":[{"S":"semi-supervised learning"},{"S":" privacy-preserving learning"}]},"sections":{"M":{"why":{"S":"This paper provides a new method for semi-supervised learning. Semi-supervised learning methods provide ways to use unlabeled data in addition to the small amount of labeled data that is available, during training. This is often achieved by adding a loss term which leverages information from unlabeled data, during training. (But not always -- see for example the \\"Big Unsupervised models are strong semi-supervised learners\\" paper which take a multi-step approach to handling unlabeled and labeled data.)"},"how":{"S":"To obtain a batch of data for a training step, they do the following:\\n- They augment the examples from the batch, a batch from their labeled dataset and from their unlabeled dataset. Note that a labeled example is augmented once, but an unlabeled example is augmented K times.\\n- For each K augmented unlabeled example, they obtain a label guess from the network that is being trained by feeding that example into the network. They average these K predictions to obtain an average guess label. \\n- The then \u201csharpen\u201d the distribution that is obtained from these guessed models, by reducing the entropy of the guessed predictions. This provides a new, refined, guessed probability for an unlabeled example.\\n- They then apply the MixUp algorithm (a regularizer used in supervised learning) which creates convex combinations of examples and corresponding convex combinations of their labels. Note that they do not mix labeled and unlabeled examples together. The \u201cground-truth\u201d probabilities of the unlabeled examples are those obtained from the previous step. \\n\\nWith these two batches of ground-truth labels and guess labels on labeled and unlabeled data, they apply the following loss function for semi-supervised learning: (1) a cross-entropy term on the labeled dataset X\u2019 obtained from MixMatch and (2) a L2 norm that forces prediction of the neural network to be close to the final guessed labels U\u2019 obtained from the previous MixMatch batch. The latter loss term enforces the fact that the class predicted for an unlabeled example should be similar if that unlabeled example is augmented.\\n\\nOnce the loss function is computed, they update the weights of the network accordingly, and go back to doing MixMatch on the next batch to generate a new batch of labeled and unlabeled data for the next training step. "},"what":{"S":"The objective of using a loss term on large unlabeled datasets is to improve performance, reduce risk of overfitting, and encourage generalizability of the trained model. In the literature, this loss term usually belongs to one of three categories: (1) entropy minimization to encourage the model to output confident predictions, (2) consistency regularization to encourage model to produce consistent predictions when input is perturbed and (3) generic regularization.  \\n\\nThis paper proposes a new semi-supervised approach, MixMatch, which belongs to all three of these groups. This is why the title in their paper refers to a \u201cholistic approach.\u201d The authors provide a new way to generate \u201clabeled\u201d data from unlabeled datasets and use these in a loss function. "},"results":{"S":"They use the CIFAR-10, CIFAR-100, SHVN and STL datasets for their experiments. For each dataset, they use a small portion as labeled data and keep the rest as unlabeled. Their architecture is a ResNet with varying depth depending on the dataset they tackle. They compare their methods with other semi-supervised learning methods and find that their MIxMatch approach outperforms or competes well with the others.\\n\\nThey have an interesting additional experiment in their paper about privacy learning. A learning algorithm is said to be differentially private if \u201cadding, modifying or removing any of its trained samples is guaranteed not to results in a statistically significant difference in the model parameters learned.\u201d They find that their method achieves a much smaller privacy loss (a degree of privacy) than the other semi-supervised methods. "}}},"id":{"S":"2b9e3fdf-0c15-4460-8842-93fb46dcedec"},"title":{"S":"MixMatch: a holistic approach to semi-supervised learning"},"authors":{"S":"Berthelot, D., Carlini, N., Goodfellow, I., Papernot, N., Oliver, A., & Raffel, C."}}],"Count":33,"ScannedCount":33,"ConsumedCapacity":null}')},211:function(e,t,a){"use strict";a.r(t);var n,i=a(0),o=a.n(i),s=a(14),r=a.n(s),h=(a(160),a(161),a(254)),l=a(248),c=a(22),d=a(255),u=a(42),p=a(9),m=a(145),f=a(18),g=a(55);!function(e){e[e.Why=0]="Why",e[e.What=1]="What",e[e.How=2]="How",e[e.Results=3]="Results"}(n||(n={}));var b={cards:[],selectedCard:"",selectedSection:n.Why,updatedCard:void 0,visibleCardsIds:[]},w=Object(g.b)({name:"cards",initialState:b,reducers:{addCard:function(e,t){var a=t.payload.card.labels.filter((function(e){return e})).map((function(e){return e.trim()}));e.cards.push(Object(f.a)(Object(f.a)({},t.payload.card),{},{labels:a}))},addVisibleCardId:function(e,t){e.visibleCardsIds.push(t.payload.visibleCardId)},removeVisibleCardId:function(e,t){e.visibleCardsIds=e.visibleCardsIds.filter((function(e){return t.payload.visibleCardId!==e}))},editCard:function(e,t){var a=e.cards.filter((function(e){return e.id!==t.payload.card.id}));e.cards=[].concat(Object(m.a)(a),[t.payload.card])},removeCard:function(e,t){e.cards=e.cards.filter((function(e){return e.id!==t.payload.id}))},replaceSelectedCard:function(e,t){e.cards=e.cards.map((function(a){return a.id!==e.selectedCard?a:t.payload.card}))},setSelectedCard:function(e,t){e.selectedCard=t.payload.id},setSelectedSection:function(e,t){e.selectedSection=t.payload.section},setUpdatedCard:function(e,t){e.updatedCard=t.payload.updatedCard},setVisibleCardIds:function(e,t){e.visibleCardsIds=t.payload.visibleCardsIds}}}),y=w.actions,v=(y.addCard,y.addVisibleCardId,y.editCard,y.removeCard,y.replaceSelectedCard,y.setSelectedCard,y.setSelectedSection,y.setVisibleCardIds,function(e){return e.cards.cards}),k=function(e){return e.cards.cards.filter((function(t){return t.id===e.cards.selectedCard}))[0]},T=function(e){return e.cards.selectedSection},S=function(e){var t=e.cards.cards.filter((function(t){return t.id===e.cards.selectedCard}))[0];if(!t)return"";switch(e.cards.selectedSection){case n.Why:return t.sections.why;case n.What:return t.sections.what;case n.How:return t.sections.how;case n.Results:return t.sections.results;default:return""}},x=function(e){return e.cards.visibleCardsIds},j=(w.reducer,a(241)),N=Object(j.a)((function(e){return{root:{flexGrow:1,marginTop:"80px"},card:{padding:e.spacing(1),textAlign:"center",color:e.palette.text.secondary},grid:{marginTop:"40px"}}})),C=a(244),I=a(247),O=a(66),A=Object(j.a)((function(e){return{root:{minWidth:400,position:"relative",textAlign:"center",height:"100%"},editAreaContainer:{position:"absolute",bottom:"5px",display:"flex",justifyContent:"space-between",paddingLeft:"0px",paddingRight:"0px"},editIcon:{color:"#6573c3"},labelContainer:{paddingBlock:"6px",display:"flex",flexWrap:"wrap",justifyContent:"center"},container:{height:"100%",margin:"12px"},dialog:{position:"absolute",top:"50px"},labels:{position:"relative",bottom:"2px",color:"#6573c3",margin:"2px"},bullet:{display:"inline-block",margin:"0 2px",transform:"scale(0.8)"},info:{fontSize:14},authors:{marginTop:12,fontSize:14},removeDialogBox:{display:"flex",justifyContent:"space-between"},title:{lineHeight:1.2}}})),L=a(256),M=a(246),z=a(245),R=["Why is this work important?","How does this work differ from other works?","What do they propose?","What are the results?"],F=Object(j.a)({arrows:{justifyContent:"space-between"},body:{marginLeft:20,marginRight:20,whiteSpace:"pre-line",overflow:"auto"},header:{backgroundColor:"#6573c3",color:"white",whiteSpace:"pre-line"},root:{minWidth:275,minHeight:350,overflow:"auto"},icon:{color:"#6573c3",marginRight:"10px",marginLeft:"10px"},text:{},title:{fontSize:14},pos:{marginTop:-70,marginBottom:24,flexGrow:1,textAlign:"center"}}),P=a(243),D=a(104),E=a.n(D),B=a(2),q=function(){var e=F(),t=Object(p.b)(),a=Object(p.c)(T),n=function(){a-1!==-1&&t(w.actions.setSelectedSection({section:a-1}))};return 0===a?Object(B.jsx)(P.a,{disabled:!0,className:e.icon,onClick:n,children:Object(B.jsx)(E.a,{fontSize:"large"})}):Object(B.jsx)(P.a,{className:e.icon,onClick:n,children:Object(B.jsx)(E.a,{fontSize:"large"})})},W=a(105),G=a.n(W),V=function(){var e=F(),t=Object(p.c)(T),a=Object(p.b)(),n=function(){t+1!==4&&a(w.actions.setSelectedSection({section:t+1}))};return 3===t?Object(B.jsx)(P.a,{disabled:!0,className:e.icon,onClick:n,children:Object(B.jsx)(G.a,{fontSize:"large"})}):Object(B.jsx)(P.a,{className:e.icon,onClick:n,children:Object(B.jsx)(G.a,{fontSize:"large"})})};function U(){var e=F(),t=Object(p.c)(k),a=Object(p.c)(S),n=Object(p.c)(T);return t&&a?Object(B.jsxs)(C.a,{className:e.root,variant:"outlined",children:[Object(B.jsx)(z.a,{align:"center",className:e.header,title:t.title,subheader:Object(B.jsx)(O.a,{children:[t.authors,"\n",t.journal,t.date].join(" ")})}),Object(B.jsxs)(M.a,{className:e.arrows,children:[Object(B.jsx)(q,{}),Object(B.jsx)(V,{})]}),Object(B.jsxs)(I.a,{children:[Object(B.jsx)(O.a,{className:e.pos,variant:"h6",component:"h2",children:R[n]}),Object(B.jsx)(O.a,{align:"justify",className:e.body,children:a})]})]}):Object(B.jsx)(o.a.Fragment,{})}var H=a(263),X=a(249),K=Object(g.b)({name:"app",initialState:{searchTerm:""},reducers:{setSearchTerm:function(e,t){e.searchTerm=t.payload.searchTerm}}}),J=(K.actions.setSearchTerm,function(e){return e.app.searchTerm}),Y=function(e){var t=e.card,a=A(),n=Object(p.b)(),i=Object(p.c)(v);return Object(B.jsx)(l.a,{className:a.labelContainer,children:t.labels.map((function(e,t){return e.length?Object(B.jsx)(X.a,{onClick:function(){return function(e){var t=[];u.forEach(i,(function(a){a.labels.includes(e)&&t.push(a.id)})),n(w.actions.setVisibleCardIds({visibleCardsIds:t})),n(K.actions.setSearchTerm({searchTerm:"label:".concat(e.toLowerCase())}))}(e)},className:a.labels,size:"small",variant:"outlined",children:e},e):Object(B.jsx)(o.a.Fragment,{},t)}))})},Z=(a(140),a(141),a(250)),Q=(a(251),a(252),a(65),a(28)),_=(a(260),a(265)),$=a(139);a.n($)()({root:{minWidth:275,minHeight:350,overflow:"auto"},textField:{margin:8}}),a(262);function ee(e){var t=e.card,a=A(),i=Object(p.b)(),s=o.a.useState(!1),r=Object(c.a)(s,2),h=r[0],u=r[1];return Object(B.jsxs)(o.a.Fragment,{children:[Object(B.jsx)(d.a,{item:!0,xs:4,children:Object(B.jsx)(l.a,{className:a.container,children:Object(B.jsxs)(C.a,{variant:"outlined",className:a.root,children:[Object(B.jsx)(Y,{card:t}),Object(B.jsx)(L.a,{onClick:function(){u(!0),i(w.actions.setSelectedCard({id:t.id})),i(w.actions.setSelectedSection({section:n.Why}))},children:Object(B.jsxs)(I.a,{children:[Object(B.jsx)(O.a,{className:a.title,align:"center",variant:"h6",component:"h2",children:t.title}),Object(B.jsx)(O.a,{className:a.authors,align:"center",color:"textSecondary",children:t.authors}),Object(B.jsx)(O.a,{className:a.info,align:"center",color:"textSecondary",gutterBottom:!0,children:[t.journal,t.date].join(", ")})]})}),!1]})})},t.id),Object(B.jsx)(H.a,{classes:{paper:a.dialog},fullWidth:!0,maxWidth:"lg",onClose:function(){u(!1),i(w.actions.setSelectedCard({id:""}))},open:h,children:Object(B.jsx)(U,{})})]})}var te=function(e){var t=e.cards,a=e.id;return Object(B.jsx)(d.a,{container:!0,item:!0,xs:12,spacing:3,children:t.map((function(e){return Object(B.jsx)(o.a.Fragment,{children:Object(B.jsx)(ee,{card:e})},e.id)}))},a)},ae=function(){var e=N(),t=Object(p.c)(v),a=Object(p.c)(x),n=Object(i.useState)(t),s=Object(c.a)(n,2),r=s[0],h=s[1];return Object(i.useEffect)((function(){h(u.filter(t,(function(e){return a.includes(e.id)})))}),[t,a]),Object(B.jsx)("div",{className:e.root,children:Object(B.jsx)(d.a,{container:!0,spacing:1,children:u.chunk(r,3).map((function(e){return Object(B.jsx)(o.a.Fragment,{children:Object(B.jsx)(te,{id:e[0].id,cards:e})},e[0].id)}))})})},ne=a(258),ie=a(259),oe=a(142),se=a.n(oe),re=a(264),he=a(257),le=a(11),ce=Object(j.a)((function(e){var t;return Object(he.a)({aboutIcon:{color:e.palette.common.white},appbar:{color:"#6573c3"},chip:{color:"white",fontSize:"large"},grow:{flexGrow:1},homeButton:{textTransform:"none"},menuButton:{marginRight:e.spacing(2)},title:(t={display:"none"},Object(Q.a)(t,e.breakpoints.up("sm"),{display:"block"}),Object(Q.a)(t,"color",e.palette.common.white),t),search:Object(Q.a)({position:"relative",borderRadius:e.shape.borderRadius,backgroundColor:Object(le.c)(e.palette.common.white,.15),"&:hover":{backgroundColor:Object(le.c)(e.palette.common.white,.25)},marginRight:e.spacing(2),marginLeft:0,width:"100%"},e.breakpoints.up("sm"),{marginLeft:e.spacing(17),width:"30%"}),searchIcon:{padding:e.spacing(0,2),height:"100%",position:"absolute",pointerEvents:"none",display:"flex",alignItems:"center",justifyContent:"center"},inputRoot:{color:"inherit"},inputInput:Object(Q.a)({padding:e.spacing(1,1,1,0),paddingLeft:"calc(1em + ".concat(e.spacing(4),"px)"),transition:e.transitions.create("width")},e.breakpoints.up("md"),{width:"55ch"})})})),de=function(){var e=ce(),t=Object(p.c)(v),a=Object(p.c)(J),n=Object(p.b)();return Object(B.jsxs)("div",{className:e.search,children:[Object(B.jsx)("div",{className:e.searchIcon,children:Object(B.jsx)(se.a,{})}),Object(B.jsx)(re.a,{value:a,placeholder:"Search\u2026",classes:{root:e.inputRoot,input:e.inputInput},inputProps:{"aria-label":"search"},onChange:function(e){var a=e.target.value;n(K.actions.setSearchTerm({searchTerm:a}));var i=[],o=a.indexOf("label:"),s="";-1!==o&&(s=a.slice(o+6)),u.forEach(t,(function(e){(e.title.toLowerCase().indexOf(a.toLowerCase())>-1||e.authors.toLowerCase().indexOf(a.toLowerCase())>-1)&&i.push(e.id),-1!==o&&s&&e.labels.map((function(e){return e.toLowerCase()})).includes(s)&&i.push(e.id)})),n(w.actions.setVisibleCardIds({visibleCardsIds:i}))}})]})},ue=a(143),pe=a.n(ue),me=Object(j.a)((function(e){return Object(he.a)({paper:{borderWidth:10,borderColor:"#6573c3",borderStyle:"solid",backgroundColor:"#fff",color:"#101010"},text:{}})})),fe=function(e){var t=e.onClose,a=e.open,n=me();return Object(B.jsx)(H.a,{maxWidth:"lg",onClose:t,open:a,children:Object(B.jsx)(Z.a,{className:n.paper,children:Object(B.jsxs)(d.a,{style:{display:"inline-block"},container:!0,children:[Object(B.jsx)(O.a,{className:n.text,children:"Hi there! Here's a bit of information about why I made this web app."}),Object(B.jsx)("br",{}),Object(B.jsx)(O.a,{className:n.text,children:"After reading a paper, I usually write down the answer to these four questions:"}),Object(B.jsx)("br",{}),Object(B.jsx)(O.a,{component:"div",children:Object(B.jsx)(_.a,{fontStyle:"italic",children:'Q1: "Why is this work important?" '})}),Object(B.jsx)(O.a,{className:n.text,children:"This gives me a sense of the application of the paper and why it is important. It helps me put things into context and understand the rest of the content of the paper better."}),Object(B.jsx)("br",{}),Object(B.jsxs)(O.a,{component:"div",children:["  ",Object(B.jsx)(_.a,{fontStyle:"italic",children:'Q2: "How is this work different from other existing works?" '})]}),Object(B.jsx)(O.a,{className:n.text,children:"This tells us what is new or different about the method proposed by the authors, when comparing against the rest of the current literature. This should point out the key contribution of the paper."}),Object(B.jsx)("br",{}),Object(B.jsx)(O.a,{component:"div",children:Object(B.jsx)(_.a,{fontStyle:"italic",children:'Q3: "What do they propose?" '})}),Object(B.jsx)(O.a,{className:n.text,children:"This answer delves into some of the technical details of the paper, although it still tries to avoid the nitty-gritty and fouses on the big picture idea."}),Object(B.jsx)("br",{}),Object(B.jsxs)(O.a,{component:"div",children:["    ",Object(B.jsx)(_.a,{fontStyle:"italic",children:'Q4: "What are the results?" '})]}),Object(B.jsx)(O.a,{className:n.text,children:"This provides the interesting or significant results that are particularly important. Any additional comments or thoughts I have would be written there as well."}),Object(B.jsx)("br",{}),Object(B.jsx)("br",{}),Object(B.jsx)(O.a,{className:n.text,children:"Whenever I need to refresh my memory about a given paper, I read the answers to these four questions. This web app allows me to store all of these summaries in a single place."})]})})})},ge=function(){var e=ce(),t=Object(i.useState)(!1),a=Object(c.a)(t,2),n=a[0],s=a[1];return Object(B.jsxs)(o.a.Fragment,{children:[Object(B.jsxs)(X.a,{onClick:function(){s(!0)},children:[Object(B.jsx)(P.a,{children:Object(B.jsx)(pe.a,{className:e.aboutIcon})}),Object(B.jsx)(O.a,{className:e.title,children:"About"})]}),Object(B.jsx)(fe,{onClose:function(){s(!1)},open:n})]})},be=function(){ce();var e=Object(i.useState)(!1),t=Object(c.a)(e,2);t[0],t[1];return Object(B.jsx)(ge,{})},we=a(266),ye=function(){var e=ce(),t=Object(p.b)(),a=Object(p.c)(x),n=Object(p.c)(v);return Object(B.jsxs)(o.a.Fragment,{children:[Object(B.jsx)(X.a,{className:e.homeButton,onClick:function(){t(K.actions.setSearchTerm({searchTerm:""}));var e=n.map((function(e){return e.id}));t(w.actions.setVisibleCardIds({visibleCardsIds:e}))},children:Object(B.jsx)(O.a,{className:e.title,variant:"h5",noWrap:!0,children:"Paper Stories"})}),a.length>0&&Object(B.jsx)(we.a,{color:"primary",size:"medium",className:e.chip,label:a.length})]})},ve=function(){var e=ce();return Object(B.jsx)("div",{className:e.grow,children:Object(B.jsx)(ne.a,{style:{background:"#6573c3"},children:Object(B.jsxs)(ie.a,{children:[Object(B.jsx)(ye,{}),Object(B.jsx)(de,{}),Object(B.jsx)("div",{className:e.grow}),Object(B.jsx)(be,{})]})})})},ke=a(144),Te=a(210),Se=Object(ke.a)({typography:{fontFamily:"'Lato'"}});var xe=function(){var e=Object(p.b)(),t=function(){Te.Items.map((function(e){var t=e.date.S,a=e.journal.S,n=e.labels.L.map((function(e){return e.S})),i=e.id.S,o=e.title.S,s=e.authors.S,r=e.sections.M.why.S,h=e.sections.M.how.S;return{id:i,authors:s,date:t,journal:a,labels:n,title:o,sections:{why:r,what:e.sections.M.what.S,how:h,results:e.sections.M.results.S}}})).forEach((function(t){e(w.actions.addCard({card:t})),e(w.actions.addVisibleCardId({visibleCardId:t.id}))}))};return Object(i.useEffect)((function(){t()}),[]),Object(B.jsx)(h.a,{theme:Se,children:Object(B.jsxs)(l.a,{maxWidth:"lg",children:[Object(B.jsx)(ve,{}),Object(B.jsx)(ae,{})]})})},je=function(e){e&&e instanceof Function&&a.e(3).then(a.bind(null,268)).then((function(t){var a=t.getCLS,n=t.getFID,i=t.getFCP,o=t.getLCP,s=t.getTTFB;a(e),n(e),i(e),o(e),s(e)}))},Ne=Object(g.a)({reducer:{cards:w.reducer,app:K.reducer}});r.a.render(Object(B.jsx)(p.a,{store:Ne,children:Object(B.jsx)(xe,{})}),document.getElementById("root")),je()}},[[211,1,2]]]);
//# sourceMappingURL=main.1c042fc2.chunk.js.map